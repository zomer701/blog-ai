{
  "id": "sample-anthropic-advanced-tool-use",
  "source": "Anthropic",
  "source_url": "https://www.anthropic.com/engineering/advanced-tool-use",
  "title": "Introducing advanced tool use on the Claude Developer Platform",
  "author": "Anthropic Engineering",
  "published_date": "2025-11-24T00:00:00Z",
  "scraped_at": 1733203200000,
  "status": "published",
  "content": {
    "original_html": "",
    "text": "The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.\n\nTo build effective agents , they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.\n\nAgents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.\n\nAgents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.\n\nToday, we're releasing three features that make this possible:\n\n- Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window\n\n- Programmatic Tool Calling , which allows Claude to invoke tools in a code execution environment reducing the impact on the model's context window\n\n- Tool Use Examples , which provides a universal standard for demonstrating how to effectively use a given tool\n\nIn internal testing, we've found these features have helped us build things that wouldn't have been possible with conventional tool use patterns. For example, Claude for Excel uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model's context window.\n\nBased on our experience, we believe these features open up new possibilities for what you can build with Claude.\n\nTool Search Tool\n\nThe challenge\n\nMCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:\n\n- GitHub: 35 tools (~26K tokens)\n\n- Slack: 11 tools (~21K tokens)\n\n- Sentry: 5 tools (~3K tokens)\n\n- Grafana: 5 tools (~3K tokens)\n\n- Splunk: 2 tools (~2K tokens)\n\nThat's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.\n\nBut token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like notification-send-user vs. notification-send-channel .\n\nOur solution\n\nInstead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.\n\n[[IMAGE_1|Tool Search Tool preserves 191,300 tokens of context compared to 122,800 with Claude's traditional approach.]]\n\nTraditional approach:\n\n- All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)\n\n- Conversation history and system prompt compete for remaining space\n\n- Total context consumption: ~77K tokens before any work begins\n\nWith the Tool Search Tool:\n\n- Only the Tool Search Tool loaded upfront (~500 tokens)\n\n- Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)\n\n- Total context consumption: ~8.7K tokens, preserving 95% of context window\n\nThis represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.\n\nHow the Tool Search Tool works\n\nThe Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with defer_loading: true to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with defer_loading: false (your most critical, frequently-used tools).\n\nWhen Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.\n\nFor example, if Claude needs to interact with GitHub, it searches for \"github,\" and only github.createPullRequest and github.listIssues get loaded-not your other 50+ tools from Slack, Jira, and Google Drive.\n\nThis way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.\n\nPrompt caching note: Tool Search Tool doesn't break prompt caching because deferred tools are excluded from the initial prompt entirely. They're only added to context after Claude searches for them, so your system prompt and core tool definitions remain cacheable.\n\nImplementation:\n\n```\n{\n  \"tools\": [\n    // Include a tool search tool (regex, BM25, or custom)\n    {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n\n    // Mark tools for on-demand discovery\n    {\n      \"name\": \"github.createPullRequest\",\n      \"description\": \"Create a pull request\",\n      \"input_schema\": {...},\n      \"defer_loading\": true\n    }\n    // ... hundreds more deferred tools with defer_loading: true\n  ]\n}\n```\n\nFor MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:\n\n```\n{\n  \"type\": \"mcp_toolset\",\n  \"mcp_server_name\": \"google-drive\",\n  \"default_config\": {\"defer_loading\": true}, # defer loading the entire server\n  \"configs\": {\n    \"search_files\": {\n\"defer_loading\": false\n    }  // Keep most used tool loaded\n  }\n}\n```\n\nThe Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.\n\nWhen to use the Tool Search Tool\n\nLike any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.\n\nUse it when:\n\n- Tool definitions consuming >10K tokens\n\n- Experiencing tool selection accuracy issues\n\n- Building MCP-powered systems with multiple servers\n\n- 10+ tools available\n\nLess beneficial when:\n\n- Small tool library (<10 tools)\n\n- All tools used frequently in every session\n\n- Tool definitions are compact\n\nProgrammatic Tool Calling\n\nThe challenge\n\nTraditional tool calling creates two fundamental problems as workflows become more complex:\n\n- Context pollution from intermediate results : When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.\n\n- Inference overhead and manual synthesis : Each tool call requires a full model inference pass. After receiving results, Claude must \"eyeball\" the data to extract relevant information, reason about how pieces fit together, and decide what to do next-all through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.\n\nOur solution\n\nProgrammatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.\n\nClaude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.\n\nExample: Budget compliance check\n\nConsider a common business task: \"Which team members exceeded their Q3 travel budget?\"\n\nYou have three tools available:\n\n- get_team_members(department) - Returns team member list with IDs and levels\n\n- get_expenses(user_id, quarter) - Returns expense line items for a user\n\n- get_budget_by_level(level) - Returns budget limits for an employee level\n\nTraditional approach :\n\n- Fetch team members 20 people\n\n- For each person, fetch their Q3 expenses 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)\n\n- Fetch budget limits by employee level\n\n- All of this enters Claude's context: 2,000+ expense line items (50 KB+)\n\n- Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits\n\n- More round-trips to the model, significant context consumption\n\nWith Programmatic Tool Calling :\n\nInstead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.\n\n[[IMAGE_2|Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips, allowing for parallel tool execution.]]\n\nHere's what Claude's orchestration code looks like for the budget compliance task:\n\n```\nteam = await get_team_members(\"engineering\")\n\n# Fetch budgets for each unique level\nlevels = list(set(m[\"level\"] for m in team))\nbudget_results = await asyncio.gather(*[\n    get_budget_by_level(level) for level in levels\n])\n\n# Create a lookup dictionary: {\"junior\": budget1, \"senior\": budget2, ...}\nbudgets = {level: budget for level, budget in zip(levels, budget_results)}\n\n# Fetch all expenses in parallel\nexpenses = await asyncio.gather(*[\n    get_expenses(m[\"id\"], \"Q3\") for m in team\n])\n\n# Find employees who exceeded their travel budget\nexceeded = []\nfor member, exp in zip(team, expenses):\n    budget = budgets[member[\"level\"]]\n    total = sum(e[\"amount\"] for e in exp)\n    if total > budget[\"travel_limit\"]:\n        exceeded.append({\n            \"name\": member[\"name\"],\n            \"spent\": total,\n            \"limit\": budget[\"travel_limit\"]\n        })\n\nprint(json.dumps(exceeded))\n```\n\nClaude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude's context, reducing consumption from 200KB of raw expense data to just 1KB of results.\n\nThe efficiency gains are substantial:\n\n- Token savings : By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.\n\n- Reduced latency : Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.\n\n- Improved accuracy : By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.\n\nProduction workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.\n\nHow Programmatic Tool Calling works\n\n1. Mark tools as callable from code\n\nAdd code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:\n\n```\n{\n  \"tools\": [\n    {\n      \"type\": \"code_execution_20250825\",\n      \"name\": \"code_execution\"\n    },\n    {\n      \"name\": \"get_team_members\",\n      \"description\": \"Get all members of a department...\",\n      \"input_schema\": {...},\n      \"allowed_callers\": [\"code_execution_20250825\"] # opt-in to programmatic tool calling\n    },\n    {\n      \"name\": \"get_expenses\",\n \t...\n    },\n    {\n      \"name\": \"get_budget_by_level\",\n\t...\n    }\n  ]\n}\n```\n\nThe API converts these tool definitions into Python functions that Claude can call.\n\n2. Claude writes orchestration code\n\nInstead of requesting tools one at a time, Claude generates Python code:\n\n```\n{\n  \"type\": \"server_tool_use\",\n  \"id\": \"srvtoolu_abc\",\n  \"name\": \"code_execution\",\n  \"input\": {\n    \"code\": \"team = get_team_members('engineering')\\n...\" # the code example above\n  }\n}\n```\n\n3. Tools execute without hitting Claude's context\n\nWhen the code calls get_expenses(), you receive a tool request with a caller field:\n\n```\n{\n  \"type\": \"tool_use\",\n  \"id\": \"toolu_xyz\",\n  \"name\": \"get_expenses\",\n  \"input\": {\"user_id\": \"emp_123\", \"quarter\": \"Q3\"},\n  \"caller\": {\n    \"type\": \"code_execution_20250825\",\n    \"tool_id\": \"srvtoolu_abc\"\n  }\n}\n```\n\nYou provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.\n\n4. Only final output enters context\n\nWhen the code finishes running, only the results of the code are returned to Claude:\n\n```\n{\n  \"type\": \"code_execution_tool_result\",\n  \"tool_use_id\": \"srvtoolu_abc\",\n  \"content\": {\n    \"stdout\": \"[{\\\"name\\\": \\\"Alice\\\", \\\"spent\\\": 12500, \\\"limit\\\": 10000}...]\"\n  }\n}\n```\n\nThis is all Claude sees, not the 2000+ expense line items processed along the way.\n\nWhen to use Programmatic Tool Calling\n\nProgrammatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.\n\nMost beneficial when:\n\n- Processing large datasets where you only need aggregates or summaries\n\n- Running multi-step workflows with three or more dependent tool calls\n\n- Filtering, sorting, or transforming tool results before Claude sees them\n\n- Handling tasks where intermediate data shouldn't influence Claude's reasoning\n\n- Running parallel operations across many items (checking 50 endpoints, for example)\n\nLess beneficial when:\n\n- Making simple single-tool invocations\n\n- Working on tasks where Claude should see and reason about all intermediate results\n\n- Running quick lookups with small responses\n\nTool Use Examples\n\nThe challenge\n\nJSON Schema excels at defining structure-types, required fields, allowed enums-but it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.\n\nConsider a support ticket API:\n\n```\n{\n  \"name\": \"create_ticket\",\n  \"input_schema\": {\n    \"properties\": {\n      \"title\": {\"type\": \"string\"},\n      \"priority\": {\"enum\": [\"low\", \"medium\", \"high\", \"critical\"]},\n      \"labels\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n      \"reporter\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\"type\": \"string\"},\n          \"name\": {\"type\": \"string\"},\n          \"contact\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"email\": {\"type\": \"string\"},\n              \"phone\": {\"type\": \"string\"}\n            }\n          }\n        }\n      },\n      \"due_date\": {\"type\": \"string\"},\n      \"escalation\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"level\": {\"type\": \"integer\"},\n          \"notify_manager\": {\"type\": \"boolean\"},\n          \"sla_hours\": {\"type\": \"integer\"}\n        }\n      }\n    },\n    \"required\": [\"title\"]\n  }\n}\n```\n\nThe schema defines what's valid, but leaves critical questions unanswered:\n\n- Format ambiguity: Should due_date use \"2024-11-06\", \"Nov 6, 2024\", or \"2024-11-06T00:00:00Z\"?\n\n- ID conventions: Is reporter.id a UUID, \"USR-12345\", or just \"12345\"?\n\n- Nested structure usage: When should Claude populate reporter.contact ?\n\n- Parameter correlations: How do escalation.level and escalation.sla_hours relate to priority?\n\nThese ambiguities can lead to malformed tool calls and inconsistent parameter usage.\n\nOur solution\n\nTool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:\n\n```\n{\n    \"name\": \"create_ticket\",\n    \"input_schema\": { /* same schema as above */ },\n    \"input_examples\": [\n      {\n        \"title\": \"Login page returns 500 error\",\n        \"priority\": \"critical\",\n        \"labels\": [\"bug\", \"authentication\", \"production\"],\n        \"reporter\": {\n          \"id\": \"USR-12345\",\n          \"name\": \"Jane Smith\",\n          \"contact\": {\n            \"email\": \"jane@acme.com\",\n            \"phone\": \"+1-555-0123\"\n          }\n        },\n        \"due_date\": \"2024-11-06\",\n        \"escalation\": {\n          \"level\": 2,\n          \"notify_manager\": true,\n          \"sla_hours\": 4\n        }\n      },\n      {\n        \"title\": \"Add dark mode support\",\n        \"labels\": [\"feature-request\", \"ui\"],\n        \"reporter\": {\n          \"id\": \"USR-67890\",\n          \"name\": \"Alex Chen\"\n        }\n      },\n      {\n        \"title\": \"Update API documentation\"\n      }\n    ]\n  }\n```\n\nFrom these three examples, Claude learns:\n\n- Format conventions : Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case\n\n- Nested structure patterns : How to construct the reporter object with its nested contact object\n\n- Optional parameter correlations : Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only\n\nIn our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.\n\nWhen to use Tool Use Examples\n\nTool Use Examples add tokens to your tool definitions, so they're most valuable when accuracy improvements outweigh the additional cost.\n\nMost beneficial when:\n\n- Complex nested structures where valid JSON doesn't imply correct usage\n\n- Tools with many optional parameters and inclusion patterns matter\n\n- APIs with domain-specific conventions not captured in schemas\n\n- Similar tools where examples clarify which one to use (e.g., create_ticket vs create_incident )\n\nLess beneficial when:\n\n- Simple single-parameter tools with obvious usage\n\n- Standard formats like URLs or emails that Claude already understands\n\n- Validation concerns better handled by JSON Schema constraints\n\nBest practices\n\nBuilding agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.\n\nLayer features strategically\n\nNot every agent needs to use all three features for a given task. Start with your biggest bottleneck:\n\n- Context bloat from tool definitions Tool Search Tool\n\n- Large intermediate results polluting context Programmatic Tool Calling\n\n- Parameter errors and malformed calls Tool Use Examples\n\nThis focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.\n\nThen layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.\n\nSet up Tool Search Tool for better discovery\n\nTool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.\n\n```\n// Good\n{\n    \"name\": \"search_customer_orders\",\n    \"description\": \"Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info.\"\n}\n\n// Bad\n{\n    \"name\": \"query_db_orders\",\n    \"description\": \"Execute order query\"\n}\n```\n\nAdd system prompt guidance so Claude knows what's available:\n\n```\nYou have access to tools for Slack messaging, Google Drive file management, \nJira ticket tracking, and GitHub repository operations. Use the tool search \nto find specific capabilities.\n```\n\nKeep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.\n\nSet up Programmatic Tool Calling for correct execution\n\nSince Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:\n\n```\n{\n    \"name\": \"get_orders\",\n    \"description\": \"Retrieve orders for a customer.\nReturns:\n    List of order objects, each containing:\n    - id (str): Order identifier\n    - total (float): Order total in USD\n    - status (str): One of 'pending', 'shipped', 'delivered'\n    - items (list): Array of {sku, quantity, price}\n    - created_at (str): ISO 8601 timestamp\"\n}\n```\n\nSee below for opt-in tools that benefit from programmatic orchestration:\n\n- Tools that can run in parallel (independent operations)\n\n- Operations safe to retry (idempotent)\n\nSet up Tool Use Examples for parameter accuracy\n\nCraft examples for behavioral clarity:\n\n- Use realistic data (real city names, plausible prices, not \"string\" or \"value\")\n\n- Show variety with minimal, partial, and full specification patterns\n\n- Keep it concise: 1-5 examples per tool\n\n- Focus on ambiguity (only add examples where correct usage isn't obvious from schema)\n\nGetting started\n\nThese features are available in beta. To enable them, add the beta header and include the tools you need:\n\n```\nclient.beta.messages.create(\n    betas=[\"advanced-tool-use-2025-11-20\"],\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=4096,\n    tools=[\n        {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n        {\"type\": \"code_execution_20250825\", \"name\": \"code_execution\"},\n        # Your tools with defer_loading, allowed_callers, and input_examples\n    ]\n)\n```\n\nFor detailed API documentation and SDK examples, see our:\n\n- D ocumentation and cookbook for Tool Search Tool\n\n- Documentation and cookbook for Programmatic Tool Calling\n\n- Documentation for Tool Use Examples\n\nThese features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.\n\nWe're excited to see what you build.\n\nAcknowledgements\n\nWritten by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM , Cloudflare's Code Mode and Code Execution as MCP . Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.",
    "images": [
      "https://www-cdn.anthropic.com/images/4zrzovbb/website/151600be7f9c23247aad8dcb6aacb2e1ab024f44-1000x1000.svg",
      "https://www-cdn.anthropic.com/images/4zrzovbb/website/f359296f770706608901eadaffbff4ca0b67874c-1999x1125.png",
      "https://www-cdn.anthropic.com/images/4zrzovbb/website/65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png"
    ]
  },
  "translations": {
    "es": {
      "title": "Nuevas funciones de uso avanzado de herramientas en Claude",
      "content": "El futuro de los agentes de IA es que trabajen sin fricciones con cientos o miles de herramientas. Un asistente de IDE que integra operaciones de git, manejo de archivos, gestores de paquetes, pruebas y despliegues. Un coordinador de operaciones que conecta Slack, GitHub, Google Drive, Jira, bases de datos y docenas de servidores MCP al mismo tiempo.\n\nPara construir agentes eficaces, deben trabajar con bibliotecas ilimitadas de herramientas sin cargar todas las definiciones en contexto. En nuestro artículo sobre ejecución de código con MCP mostramos cómo los resultados y definiciones pueden consumir más de 50 000 tokens antes de que el agente lea una petición. Los agentes deberían descubrir y cargar herramientas bajo demanda, manteniendo solo lo relevante para la tarea.\n\nTambién necesitan invocar herramientas desde código. Con llamadas en lenguaje natural cada invocación requiere una pasada completa del modelo y los resultados intermedios se acumulan en contexto aunque no sean útiles. El código se adapta mejor a bucles, condicionales y transformaciones. Los agentes necesitan elegir entre ejecución de código o inferencia según la tarea.\n\nLas herramientas deben aprenderse con ejemplos, no solo con esquemas. Los JSON schemas definen estructura, pero no patrones de uso: cuándo incluir opcionales, qué combinaciones tienen sentido o qué convenciones espera tu API.\n\nHoy lanzamos tres funciones que habilitan esto:\n\n- Tool Search Tool, para que Claude busque herramientas sin consumir la ventana de contexto\n\n- Programmatic Tool Calling, para invocar herramientas en un entorno de ejecución de código reduciendo el impacto en el contexto\n\n- Tool Use Examples, un estándar universal para mostrar cómo usar cada herramienta\n\nEn pruebas internas, estas funciones nos ayudaron a construir cosas imposibles con el patrón tradicional. Por ejemplo, Claude for Excel usa llamadas programáticas para leer y modificar hojas con miles de filas sin saturar el contexto.\n\nCreemos que estas funciones abren nuevas posibilidades para lo que puedes construir con Claude.\n\nTool Search Tool\n\nEl desafío\n\nLas definiciones de herramientas MCP suman muchos tokens. En un entorno con cinco servidores:\n\n- GitHub: 35 herramientas (~26K tokens)\n\n- Slack: 11 herramientas (~21K tokens)\n\n- Sentry: 5 herramientas (~3K tokens)\n\n- Grafana: 5 herramientas (~3K tokens)\n\n- Splunk: 2 herramientas (~2K tokens)\n\nSon ~55K tokens antes de empezar. Con más servidores como Jira (~17K tokens) superas 100K. Hemos visto casos de 134K tokens antes de optimizar.\n\nEl problema no es solo el costo: los fallos más comunes son la selección incorrecta de herramientas y parámetros (notification-send-user vs. notification-send-channel).\n\nNuestra solución\n\nEn vez de cargar todas las definiciones, Tool Search Tool las descubre bajo demanda. Claude solo ve lo que necesita para la tarea.\n\n[[IMAGE_1|Tool Search Tool preserva 191 300 tokens de contexto frente a 122 800 con el enfoque tradicional.]]\n\nEnfoque tradicional:\n\n- Todas las definiciones cargadas (~72K tokens para 50+ herramientas)\n\n- El historial y el system prompt compiten por el espacio restante\n\n- Consumo total: ~77K tokens antes de trabajar\n\nCon Tool Search Tool:\n\n- Solo se carga la herramienta de búsqueda (~500 tokens)\n\n- Herramientas descubiertas bajo demanda (3-5 relevantes, ~3K tokens)\n\n- Consumo total: ~8.7K tokens, preservando 95 % de la ventana\n\nEsto reduce tokens en 85 % y mantiene acceso a toda la biblioteca. Las evaluaciones MCP mejoraron: Opus 4 pasó de 49 % a 74 %, Opus 4.5 de 79.5 % a 88.1 % con Tool Search Tool.\n\nCómo funciona\n\nTool Search Tool permite descubrir herramientas dinámicamente. Proporcionas todas tus definiciones, pero marcas con defer_loading: true las que deben cargarse bajo demanda. Claude solo ve la propia herramienta de búsqueda y las marcadas como defer_loading: false.\n\nCuando Claude necesita capacidades, busca herramientas. Tool Search Tool devuelve referencias que se expanden a definiciones completas en contexto.\n\nPor ejemplo, si busca \"github\", solo se cargan github.createPullRequest y github.listIssues, no las otras 50+ de Slack, Jira o Google Drive.\n\nAsí, Claude accede a toda la biblioteca pagando solo por lo que usa.\n\nNota de caché: al excluir las herramientas diferidas del prompt inicial, el caché se mantiene.\n\nImplementación:\n\n```\n{\n  \"tools\": [\n    {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n    {\n      \"name\": \"github.createPullRequest\",\n      \"description\": \"Create a pull request\",\n      \"input_schema\": {...},\n      \"defer_loading\": true\n    }\n    // ... cientos más con defer_loading: true\n  ]\n}\n```\n\nPara servidores MCP puedes diferir el servidor completo y dejar cargadas las herramientas críticas:\n\n```\n{\n  \"type\": \"mcp_toolset\",\n  \"mcp_server_name\": \"google-drive\",\n  \"default_config\": {\"defer_loading\": true},\n  \"configs\": {\n    \"search_files\": {\"defer_loading\": false}\n  }\n}\n```\n\nLa plataforma ofrece búsquedas regex y BM25 listas para usar, pero puedes implementar búsquedas personalizadas con embeddings u otras estrategias.\n\nCuándo usar Tool Search Tool\n\nÚsalo cuando el ahorro de contexto y precisión compense la latencia extra de búsqueda.\n\nÚtil cuando:\n\n- Definiciones consumen >10K tokens\n\n- Hay errores de selección de herramientas\n\n- Sistemas MCP con varios servidores\n\n- >10 herramientas disponibles\n\nMenos útil cuando:\n\n- Biblioteca pequeña (<10 herramientas)\n\n- Todas las herramientas se usan en cada sesión\n\n- Definiciones compactas\n\nProgrammatic Tool Calling\n\nEl reto\n\nEl patrón tradicional genera:\n\n- Contaminación de contexto por resultados intermedios (p.ej., un log de 10 MB entero en contexto).\n\n- Latencia e inferencias múltiples: cada llamada es una pasada del modelo y luego debe sintetizar resultados manualmente.\n\nNuestra solución\n\nPTC permite orquestar herramientas con código en vez de rondas de API. Claude escribe código que llama varias herramientas, procesa resultados y controla qué entra en contexto.\n\nClaude es bueno escribiendo código; así se obtiene control explícito: bucles, condicionales, transformaciones, manejo de errores.\n\nEjemplo: control de presupuesto\n\nHerramientas:\n\n- get_team_members(department)\n\n- get_expenses(user_id, quarter)\n\n- get_budget_by_level(level)\n\nEnfoque tradicional:\n\n- 20 miembros → 20 llamadas de gastos (50-100 líneas cada una) + límites por nivel → 2 000+ líneas en contexto.\n\n- Claude suma, compara y decide en lenguaje natural → lento y propenso a errores.\n\nCon PTC:\n\nClaude escribe un script Python que orquesta todo en un entorno de ejecución. Pausa cuando necesita resultados, los procesa fuera del contexto y solo devuelve la salida final.\n\n[[IMAGE_2|Programmatic Tool Calling permite orquestar herramientas con código y ejecución en paralelo.]]\n\nCódigo de ejemplo para cumplimiento de presupuesto:\n\n```\nteam = await get_team_members(\"engineering\")\nlevels = list(set(m[\"level\"] for m in team))\nbudget_results = await asyncio.gather(*[\n    get_budget_by_level(level) for level in levels\n])\nbudgets = {level: budget for level, budget in zip(levels, budget_results)}\nexpenses = await asyncio.gather(*[\n    get_expenses(m[\"id\"], \"Q3\") for m in team\n])\nexceeded = []\nfor member, exp in zip(team, expenses):\n    budget = budgets[member[\"level\"]]\n    total = sum(e[\"amount\"] for e in exp)\n    if total > budget[\"travel_limit\"]:\n        exceeded.append({\n            \"name\": member[\"name\"],\n            \"spent\": total,\n            \"limit\": budget[\"travel_limit\"]\n        })\nprint(json.dumps(exceeded))\n```\n\nSolo el resultado final entra en contexto (2-3 personas). Los 2 000+ items y sumas no consumen contexto: de 200 KB a ~1 KB.\n\nBeneficios:\n\n- Ahorro de tokens: -37 % en tareas complejas (43 588 → 27 297)\n\n- Menos latencia: eliminas muchas pasadas de inferencia.\n\n- Mayor precisión: lógica explícita reduce errores; mejoras en retrieval y benchmarks GIA.\n\nCómo funciona\n\n1) Marcar herramientas para ejecución desde código (allowed_callers con code_execution).\n\n2) Claude escribe código (server_tool_use) que llama a las herramientas.\n\n3) Las herramientas se ejecutan fuera del contexto del modelo; cada tool_use incluye caller.\n\n4) Solo se devuelve la salida final (code_execution_tool_result).\n\nCuándo usar PTC\n\nMás valioso cuando:\n\n- Datos grandes donde solo necesitas agregados o resúmenes\n\n- Flujos de varios pasos dependientes\n\n- Necesitas filtrar/transformar antes de mostrar al modelo\n\n- Datos intermedios no deben influir en el razonamiento\n\n- Operaciones paralelas a muchos elementos\n\nMenos útil cuando:\n\n- Llamadas simples de una herramienta\n\n- Necesitas que el modelo vea todos los resultados intermedios\n\nTool Use Examples\n\nEl reto\n\nJSON Schema define estructura pero no patrones de uso: fechas, IDs, opcionales, correlaciones.\n\nSolución: ejemplos en las definiciones para mostrar formato, anidación y parámetros opcionales.\n\nEjemplo create_ticket con input_examples enseña:\n\n- Fechas YYYY-MM-DD, IDs USR-XXXXX, labels en kebab-case\n\n- Cómo construir reporter/contact\n\n- Cuándo incluir escalations\n\nResultados: de 72 % a 90 % de precisión en parámetros complejos.\n\nCuándo usar ejemplos\n\nMás útil cuando hay estructuras anidadas, muchos opcionales, convenciones de dominio o herramientas parecidas. Menos útil con parámetros simples o formatos estándar.\n\nBuenas prácticas\n\n1) Usa descripciones claras para mejorar la búsqueda de herramientas.\n\n2) Documenta formatos de salida para que Claude parsee bien.\n\n3) Añade ejemplos concisos (1-5) con datos realistas.\n\n4) Empieza con el cuello de botella principal (contexto, resultados, parámetros) y luego combina las tres funciones.\n\nConfiguración\n\n```\nclient.beta.messages.create(\n  betas=[\"advanced-tool-use-2025-11-20\"],\n  model=\"claude-sonnet-4-5-20250929\",\n  max_tokens=4096,\n  tools=[\n    {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n    {\"type\": \"code_execution_20250825\", \"name\": \"code_execution\"},\n    # Tus herramientas con defer_loading, allowed_callers e input_examples\n  ]\n)\n```\n\nConsulta la documentación y cookbooks para cada función.\n\nEstas capacidades mueven el uso de herramientas hacia una orquestación inteligente: descubrimiento dinámico, ejecución eficiente e invocación fiable.\n\nEstamos deseando ver lo que construyes.\n\nAgradecimientos\n\nEscrito por Bin Wu, con contribuciones de Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang y el equipo de Claude Developer Platform. Basado en investigación de Chris Gorgolewski, Daniel Jiang, Jeremy Fox y Mike Lambert. Inspirado también por Joel Pobar (LLMVM), Cloudflare (Code Mode) y Code Execution as MCP. Gracias a Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer y Molly Vorwerck por su apoyo.",
      "edited": false
    },
    "ukr": {
      "title": "Розширений контроль над інструментами у Claude",
      "content": "Майбутнє агентів ШІ – безшовна робота з сотнями чи тисячами інструментів. Асистент IDE, що інтегрує git, файли, пакети, тести й деплой. Координатор операцій, що з'єднує Slack, GitHub, Google Drive, Jira, бази даних і десятки серверів MCP одночасно.\n\nЩоб агенти були ефективними, їм потрібні необмежені бібліотеки інструментів без завантаження всіх визначень у контекст. У статті про виконання коду з MCP ми показали, що результати й визначення можуть забирати 50 000+ токенів до того, як агент прочитає запит. Агенти повинні знаходити й підвантажувати інструменти за потребою, тримаючи тільки релевантне.\n\nЇм також потрібні виклики інструментів з коду. При мовних викликах кожна інвокація – це повний прогін моделі, а проміжні результати засмічують контекст. Код краще підходить для циклів, умов і трансформацій. Агенту потрібен вибір між виконанням коду та інференсом залежно від задачі.\n\nІнструменти треба «вчити» прикладами, а не лише схемами. JSON-schema описує структуру, але не патерни використання: коли додавати опції, які комбінації мають сенс, які конвенції очікує API.\n\nМи запускаємо три функції, що роблять це можливим:\n\n- Tool Search Tool, щоб Claude шукав інструменти без зайвих токенів\n\n- Programmatic Tool Calling, щоб викликати інструменти у середовищі виконання коду та не роздувати контекст\n\n- Tool Use Examples, універсальний спосіб показати, як правильно викликати інструмент\n\nУ внутрішніх тестах ці функції дали те, що було неможливо в старій моделі. Наприклад, Claude for Excel читає/редагує тисячі рядків без переповнення контексту завдяки програмним викликам.\n\nМи вважаємо, що це відкриває нові можливості для того, що можна побудувати з Claude.\n\nTool Search Tool\n\nВиклик\n\nВизначення інструментів MCP швидко ростуть. При п'яти серверах:\n\n- GitHub: 35 інструментів (~26K токенів)\n\n- Slack: 11 інструментів (~21K токенів)\n\n- Sentry: 5 інструментів (~3K токенів)\n\n- Grafana: 5 інструментів (~3K токенів)\n\n- Splunk: 2 інструменти (~2K токенів)\n\nРазом ~55K токенів до початку роботи. З додатковими серверами, як-от Jira (~17K токенів), швидко перевалюєш за 100K. Ми бачили 134K токенів до оптимізації.\n\nПроблема не лише у вартості: часті помилки – неправильний вибір інструменту чи параметрів (notification-send-user vs. notification-send-channel).\n\nРішення\n\nЗамість завантаження всіх визначень Tool Search Tool відкриває їх за потребою. Claude бачить тільки потрібні для задачі.\n\n[[IMAGE_1|Tool Search Tool зберігає 191 300 токенів контексту проти 122 800 у традиційному підході.]]\n\nТрадиційно:\n\n- Усі визначення завантажені (~72K токенів для 50+ інструментів)\n\n- Історія й system prompt конкурують за місце\n\n- Всього ~77K токенів до початку роботи\n\nЗ Tool Search Tool:\n\n- Завантажено лише інструмент пошуку (~500 токенів)\n\n- Інструменти знаходяться за потребою (3-5 релевантних, ~3K токенів)\n\n- Всього ~8.7K токенів, економія 95 % вікна контексту\n\nЦе 85 % економії токенів при збереженні доступу до всієї бібліотеки. Оцінки MCP покращилися: Opus 4 з 49 % до 74 %, Opus 4.5 з 79.5 % до 88.1 % із Tool Search Tool.\n\nЯк це працює\n\nВи надаєте всі визначення, але ставите defer_loading: true для тих, що підвантажуються на вимогу. Claude бачить лише інструмент пошуку та визначені як defer_loading: false.\n\nКоли Claude потрібні можливості, він шукає інструменти. Tool Search Tool повертає посилання, які розгортаються у повні визначення в контексті.\n\nНаприклад, при пошуку \"github\" завантажуються тільки github.createPullRequest та github.listIssues, а не ще 50+ зі Slack, Jira чи Google Drive.\n\nТак Claude має всю бібліотеку, але платить тільки за те, що використовує.\n\nКешування: інструменти, що відкладаються, відсутні у початковому prompt, тому кеш зберігається.\n\nРеалізація:\n\n```\n{\n  \"tools\": [\n    {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n    {\n      \"name\": \"github.createPullRequest\",\n      \"description\": \"Create a pull request\",\n      \"input_schema\": {...},\n      \"defer_loading\": true\n    }\n    // ... сотні інших з defer_loading: true\n  ]\n}\n```\n\nДля серверів MCP можна відкладати весь сервер, залишаючи найуживаніші інструменти активними:\n\n```\n{\n  \"type\": \"mcp_toolset\",\n  \"mcp_server_name\": \"google-drive\",\n  \"default_config\": {\"defer_loading\": true},\n  \"configs\": {\"search_files\": {\"defer_loading\": false}}\n}\n```\n\nПлатформа дає готові пошуки regex і BM25; можна додати власні (embeddings тощо).\n\nКоли використовувати Tool Search Tool\n\nВикористовуйте, якщо економія контексту/точність переважає латентність пошуку.\n\nКорисно коли:\n\n- Визначення >10K токенів\n\n- Є помилки вибору інструментів\n\n- Кілька серверів MCP\n\n- >10 інструментів доступно\n\nМенш корисно коли:\n\n- Бібліотека мала (<10)\n\n- Усі інструменти використовуються щоразу\n\n- Визначення компактні\n\nProgrammatic Tool Calling\n\nПроблема\n\nТрадиційні виклики спричиняють:\n\n- Засмічення контексту проміжними результатами (лог 10 МБ повністю у вікні).\n\n- Багато інференсів і ручний синтез: кожен виклик – прогін моделі, потім ручне «зіставлення» результатів.\n\nРішення\n\nPTC дозволяє оркестрацію інструментів кодом, а не серією викликів. Claude пише код, який викликає кілька інструментів, обробляє результати та контролює, що потрапляє у контекст.\n\nClaude добре пише код, тож логіка стає явною: цикли, умови, трансформації, обробка помилок.\n\nПриклад: контроль бюджету\n\nІнструменти:\n\n- get_team_members(department)\n\n- get_expenses(user_id, quarter)\n\n- get_budget_by_level(level)\n\nТрадиційно:\n\n- 20 людей → 20 викликів витрат (50-100 рядків кожен) + межі бюджету → 2 000+ рядків у контексті.\n\n- Claude все сумує й порівнює у мовному режимі → повільно, з помилками.\n\nЗ PTC:\n\nClaude пише Python-скрипт, що працює у середовищі виконання. Він зупиняється для результатів, обробляє їх поза контекстом і повертає тільки фінальну відповідь.\n\n[[IMAGE_2|Programmatic Tool Calling дозволяє оркеструвати інструменти кодом і виконувати паралельно.]]\n\nПриклад коду:\n\n```\nteam = await get_team_members(\"engineering\")\nlevels = list(set(m[\"level\"] for m in team))\nbudget_results = await asyncio.gather(*[\n    get_budget_by_level(level) for level in levels\n])\nbudgets = {level: budget for level, budget in zip(levels, budget_results)}\nexpenses = await asyncio.gather(*[\n    get_expenses(m[\"id\"], \"Q3\") for m in team\n])\nexceeded = []\nfor member, exp in zip(team, expenses):\n    budget = budgets[member[\"level\"]]\n    total = sum(e[\"amount\"] for e in exp)\n    if total > budget[\"travel_limit\"]:\n        exceeded.append({\n            \"name\": member[\"name\"],\n            \"spent\": total,\n            \"limit\": budget[\"travel_limit\"]\n        })\nprint(json.dumps(exceeded))\n```\n\nУ контекст потрапляє тільки фінальний результат (2-3 людини). 2 000+ елементів та проміжні суми не споживають контекст (200 КБ → ~1 КБ).\n\nПереваги:\n\n- Економія токенів: -37 % у складних задачах (43 588 → 27 297)\n\n- Менша затримка: менше прогонів моделі.\n\n- Точність: явна логіка → менше помилок; кращий retrieval і GIA.\n\nЯк це працює\n\n1) Позначити інструменти, які можна викликати з коду (allowed_callers + code_execution).\n\n2) Claude пише код (server_tool_use), що викликає інструменти.\n\n3) Інструменти виконуються поза контекстом моделі; tool_use містить caller.\n\n4) Повертається лише фінальний результат (code_execution_tool_result).\n\nКоли використовувати PTC\n\nНайкраще коли:\n\n- Великі дані, потрібні агрегати/резюме\n\n- Багатокрокові залежні флоу\n\n- Потрібно фільтрувати/трансформувати перед показом моделі\n\n- Проміжні дані не повинні впливати на reasoning\n\n- Паралельні операції на багатьох елементах\n\nМенш доречно коли:\n\n- Прості одноінструментні виклики\n\n- Модель має бачити всі проміжні результати\n\nTool Use Examples\n\nПроблема\n\nJSON Schema описує структуру, але не патерни використання: формати дат, ID, опції, кореляції.\n\nРішення: input_examples у визначеннях, щоб показати формат, вкладеність і опціональні параметри.\n\nПриклад create_ticket з input_examples вчить:\n\n- Формат YYYY-MM-DD, ID USR-XXXXX, ярлики в kebab-case\n\n- Як будувати reporter/contact\n\n- Коли додавати escalation\n\nРезультат: точність параметрів зросла з 72 % до 90 % у складних випадках.\n\nКоли додавати приклади\n\nКорисно при складних вкладених структурах, багатьох опціях, доменних конвенціях чи схожих інструментах. Менш корисно з простими параметрами або стандартними форматами.\n\nНайкращі практики\n\n1) Чіткі описи для кращого пошуку інструментів.\n\n2) Документуйте формати виходу, щоб Claude правильно парсив.\n\n3) Додавайте стислий набір прикладів (1-5) з реалістичними даними.\n\n4) Почніть із головного вузького місця (контекст, результати, параметри), потім комбінуйте всі три функції.\n\nНалаштування\n\n```\nclient.beta.messages.create(\n  betas=[\"advanced-tool-use-2025-11-20\"],\n  model=\"claude-sonnet-4-5-20250929\",\n  max_tokens=4096,\n  tools=[\n    {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n    {\"type\": \"code_execution_20250825\", \"name\": \"code_execution\"},\n    # Ваші інструменти з defer_loading, allowed_callers, input_examples\n  ]\n)\n```\n\nДеталі – у документації й cookbooks для кожної функції.\n\nЦі можливості рухають роботу з інструментами до розумної оркестрації: динамічне відкриття, ефективне виконання та надійна інвокація.\n\nМи раді побачити, що ви створите.\n\nПодяки\n\nАвтор: Bin Wu. Учасники: Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang та команда Claude Developer Platform. Базується на дослідженнях Chris Gorgolewski, Daniel Jiang, Jeremy Fox, Mike Lambert. Натхнення від Joel Pobar (LLMVM), Cloudflare (Code Mode) і Code Execution as MCP. Дякуємо Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer і Molly Vorwerck за підтримку.",
      "edited": false
    }
  },
  "metadata": {
    "tags": [
      "Anthropic",
      "MCP",
      "Tooling"
    ]
  }
}
