[
  {
    "id": "sample-anthropic-advanced-tool-use",
    "source": "Anthropic",
    "source_url": "https://www.anthropic.com/engineering/advanced-tool-use",
    "title": "Introducing advanced tool use on the Claude Developer Platform",
    "author": "Anthropic Engineering",
    "published_date": "2025-11-24T00:00:00Z",
    "scraped_at": 1733203200000,
    "status": "published",
    "content": {
      "original_html": "",
      "text": "The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.\n\nTo build effective agents , they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.\n\nAgents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.\n\nAgents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.\n\nToday, we're releasing three features that make this possible:\n\n- Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window\n\n- Programmatic Tool Calling , which allows Claude to invoke tools in a code execution environment reducing the impact on the model's context window\n\n- Tool Use Examples , which provides a universal standard for demonstrating how to effectively use a given tool\n\nIn internal testing, we've found these features have helped us build things that wouldn't have been possible with conventional tool use patterns. For example, Claude for Excel uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model's context window.\n\nBased on our experience, we believe these features open up new possibilities for what you can build with Claude.\n\nTool Search Tool\n\nThe challenge\n\nMCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:\n\n- GitHub: 35 tools (~26K tokens)\n\n- Slack: 11 tools (~21K tokens)\n\n- Sentry: 5 tools (~3K tokens)\n\n- Grafana: 5 tools (~3K tokens)\n\n- Splunk: 2 tools (~2K tokens)\n\nThat's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.\n\nBut token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like notification-send-user vs. notification-send-channel .\n\nOur solution\n\nInstead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.\n\n[[IMAGE_1|Tool Search Tool preserves 191,300 tokens of context compared to 122,800 with Claude's traditional approach.]]\n\nTraditional approach:\n\n- All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)\n\n- Conversation history and system prompt compete for remaining space\n\n- Total context consumption: ~77K tokens before any work begins\n\nWith the Tool Search Tool:\n\n- Only the Tool Search Tool loaded upfront (~500 tokens)\n\n- Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)\n\n- Total context consumption: ~8.7K tokens, preserving 95% of context window\n\nThis represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.\n\nHow the Tool Search Tool works\n\nThe Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with defer_loading: true to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with defer_loading: false (your most critical, frequently-used tools).\n\nWhen Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.\n\nFor example, if Claude needs to interact with GitHub, it searches for \"github,\" and only github.createPullRequest and github.listIssues get loaded-not your other 50+ tools from Slack, Jira, and Google Drive.\n\nThis way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.\n\nPrompt caching note: Tool Search Tool doesn't break prompt caching because deferred tools are excluded from the initial prompt entirely. They're only added to context after Claude searches for them, so your system prompt and core tool definitions remain cacheable.\n\nImplementation:\n\n```\n{\n  \"tools\": [\n    // Include a tool search tool (regex, BM25, or custom)\n    {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n\n    // Mark tools for on-demand discovery\n    {\n      \"name\": \"github.createPullRequest\",\n      \"description\": \"Create a pull request\",\n      \"input_schema\": {...},\n      \"defer_loading\": true\n    }\n    // ... hundreds more deferred tools with defer_loading: true\n  ]\n}\n```\n\nFor MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:\n\n```\n{\n  \"type\": \"mcp_toolset\",\n  \"mcp_server_name\": \"google-drive\",\n  \"default_config\": {\"defer_loading\": true}, # defer loading the entire server\n  \"configs\": {\n    \"search_files\": {\n\"defer_loading\": false\n    }  // Keep most used tool loaded\n  }\n}\n```\n\nThe Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.\n\nWhen to use the Tool Search Tool\n\nLike any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.\n\nUse it when:\n\n- Tool definitions consuming >10K tokens\n\n- Experiencing tool selection accuracy issues\n\n- Building MCP-powered systems with multiple servers\n\n- 10+ tools available\n\nLess beneficial when:\n\n- Small tool library (<10 tools)\n\n- All tools used frequently in every session\n\n- Tool definitions are compact\n\nProgrammatic Tool Calling\n\nThe challenge\n\nTraditional tool calling creates two fundamental problems as workflows become more complex:\n\n- Context pollution from intermediate results : When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.\n\n- Inference overhead and manual synthesis : Each tool call requires a full model inference pass. After receiving results, Claude must \"eyeball\" the data to extract relevant information, reason about how pieces fit together, and decide what to do next-all through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.\n\nOur solution\n\nProgrammatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.\n\nClaude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.\n\nExample: Budget compliance check\n\nConsider a common business task: \"Which team members exceeded their Q3 travel budget?\"\n\nYou have three tools available:\n\n- get_team_members(department) - Returns team member list with IDs and levels\n\n- get_expenses(user_id, quarter) - Returns expense line items for a user\n\n- get_budget_by_level(level) - Returns budget limits for an employee level\n\nTraditional approach :\n\n- Fetch team members 20 people\n\n- For each person, fetch their Q3 expenses 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)\n\n- Fetch budget limits by employee level\n\n- All of this enters Claude's context: 2,000+ expense line items (50 KB+)\n\n- Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits\n\n- More round-trips to the model, significant context consumption\n\nWith Programmatic Tool Calling :\n\nInstead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.\n\n[[IMAGE_2|Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips, allowing for parallel tool execution.]]\n\nHere's what Claude's orchestration code looks like for the budget compliance task:\n\n```\nteam = await get_team_members(\"engineering\")\n\n# Fetch budgets for each unique level\nlevels = list(set(m[\"level\"] for m in team))\nbudget_results = await asyncio.gather(*[\n    get_budget_by_level(level) for level in levels\n])\n\n# Create a lookup dictionary: {\"junior\": budget1, \"senior\": budget2, ...}\nbudgets = {level: budget for level, budget in zip(levels, budget_results)}\n\n# Fetch all expenses in parallel\nexpenses = await asyncio.gather(*[\n    get_expenses(m[\"id\"], \"Q3\") for m in team\n])\n\n# Find employees who exceeded their travel budget\nexceeded = []\nfor member, exp in zip(team, expenses):\n    budget = budgets[member[\"level\"]]\n    total = sum(e[\"amount\"] for e in exp)\n    if total > budget[\"travel_limit\"]:\n        exceeded.append({\n            \"name\": member[\"name\"],\n            \"spent\": total,\n            \"limit\": budget[\"travel_limit\"]\n        })\n\nprint(json.dumps(exceeded))\n```\n\nClaude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude's context, reducing consumption from 200KB of raw expense data to just 1KB of results.\n\nThe efficiency gains are substantial:\n\n- Token savings : By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.\n\n- Reduced latency : Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.\n\n- Improved accuracy : By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.\n\nProduction workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.\n\nHow Programmatic Tool Calling works\n\n1. Mark tools as callable from code\n\nAdd code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:\n\n```\n{\n  \"tools\": [\n    {\n      \"type\": \"code_execution_20250825\",\n      \"name\": \"code_execution\"\n    },\n    {\n      \"name\": \"get_team_members\",\n      \"description\": \"Get all members of a department...\",\n      \"input_schema\": {...},\n      \"allowed_callers\": [\"code_execution_20250825\"] # opt-in to programmatic tool calling\n    },\n    {\n      \"name\": \"get_expenses\",\n \t...\n    },\n    {\n      \"name\": \"get_budget_by_level\",\n\t...\n    }\n  ]\n}\n```\n\nThe API converts these tool definitions into Python functions that Claude can call.\n\n2. Claude writes orchestration code\n\nInstead of requesting tools one at a time, Claude generates Python code:\n\n```\n{\n  \"type\": \"server_tool_use\",\n  \"id\": \"srvtoolu_abc\",\n  \"name\": \"code_execution\",\n  \"input\": {\n    \"code\": \"team = get_team_members('engineering')\\n...\" # the code example above\n  }\n}\n```\n\n3. Tools execute without hitting Claude's context\n\nWhen the code calls get_expenses(), you receive a tool request with a caller field:\n\n```\n{\n  \"type\": \"tool_use\",\n  \"id\": \"toolu_xyz\",\n  \"name\": \"get_expenses\",\n  \"input\": {\"user_id\": \"emp_123\", \"quarter\": \"Q3\"},\n  \"caller\": {\n    \"type\": \"code_execution_20250825\",\n    \"tool_id\": \"srvtoolu_abc\"\n  }\n}\n```\n\nYou provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.\n\n4. Only final output enters context\n\nWhen the code finishes running, only the results of the code are returned to Claude:\n\n```\n{\n  \"type\": \"code_execution_tool_result\",\n  \"tool_use_id\": \"srvtoolu_abc\",\n  \"content\": {\n    \"stdout\": \"[{\\\"name\\\": \\\"Alice\\\", \\\"spent\\\": 12500, \\\"limit\\\": 10000}...]\"\n  }\n}\n```\n\nThis is all Claude sees, not the 2000+ expense line items processed along the way.\n\nWhen to use Programmatic Tool Calling\n\nProgrammatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.\n\nMost beneficial when:\n\n- Processing large datasets where you only need aggregates or summaries\n\n- Running multi-step workflows with three or more dependent tool calls\n\n- Filtering, sorting, or transforming tool results before Claude sees them\n\n- Handling tasks where intermediate data shouldn't influence Claude's reasoning\n\n- Running parallel operations across many items (checking 50 endpoints, for example)\n\nLess beneficial when:\n\n- Making simple single-tool invocations\n\n- Working on tasks where Claude should see and reason about all intermediate results\n\n- Running quick lookups with small responses\n\nTool Use Examples\n\nThe challenge\n\nJSON Schema excels at defining structure-types, required fields, allowed enums-but it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.\n\nConsider a support ticket API:\n\n```\n{\n  \"name\": \"create_ticket\",\n  \"input_schema\": {\n    \"properties\": {\n      \"title\": {\"type\": \"string\"},\n      \"priority\": {\"enum\": [\"low\", \"medium\", \"high\", \"critical\"]},\n      \"labels\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n      \"reporter\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\"type\": \"string\"},\n          \"name\": {\"type\": \"string\"},\n          \"contact\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"email\": {\"type\": \"string\"},\n              \"phone\": {\"type\": \"string\"}\n            }\n          }\n        }\n      },\n      \"due_date\": {\"type\": \"string\"},\n      \"escalation\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"level\": {\"type\": \"integer\"},\n          \"notify_manager\": {\"type\": \"boolean\"},\n          \"sla_hours\": {\"type\": \"integer\"}\n        }\n      }\n    },\n    \"required\": [\"title\"]\n  }\n}\n```\n\nThe schema defines what's valid, but leaves critical questions unanswered:\n\n- Format ambiguity: Should due_date use \"2024-11-06\", \"Nov 6, 2024\", or \"2024-11-06T00:00:00Z\"?\n\n- ID conventions: Is reporter.id a UUID, \"USR-12345\", or just \"12345\"?\n\n- Nested structure usage: When should Claude populate reporter.contact ?\n\n- Parameter correlations: How do escalation.level and escalation.sla_hours relate to priority?\n\nThese ambiguities can lead to malformed tool calls and inconsistent parameter usage.\n\nOur solution\n\nTool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:\n\n```\n{\n    \"name\": \"create_ticket\",\n    \"input_schema\": { /* same schema as above */ },\n    \"input_examples\": [\n      {\n        \"title\": \"Login page returns 500 error\",\n        \"priority\": \"critical\",\n        \"labels\": [\"bug\", \"authentication\", \"production\"],\n        \"reporter\": {\n          \"id\": \"USR-12345\",\n          \"name\": \"Jane Smith\",\n          \"contact\": {\n            \"email\": \"jane@acme.com\",\n            \"phone\": \"+1-555-0123\"\n          }\n        },\n        \"due_date\": \"2024-11-06\",\n        \"escalation\": {\n          \"level\": 2,\n          \"notify_manager\": true,\n          \"sla_hours\": 4\n        }\n      },\n      {\n        \"title\": \"Add dark mode support\",\n        \"labels\": [\"feature-request\", \"ui\"],\n        \"reporter\": {\n          \"id\": \"USR-67890\",\n          \"name\": \"Alex Chen\"\n        }\n      },\n      {\n        \"title\": \"Update API documentation\"\n      }\n    ]\n  }\n```\n\nFrom these three examples, Claude learns:\n\n- Format conventions : Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case\n\n- Nested structure patterns : How to construct the reporter object with its nested contact object\n\n- Optional parameter correlations : Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only\n\nIn our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.\n\nWhen to use Tool Use Examples\n\nTool Use Examples add tokens to your tool definitions, so they're most valuable when accuracy improvements outweigh the additional cost.\n\nMost beneficial when:\n\n- Complex nested structures where valid JSON doesn't imply correct usage\n\n- Tools with many optional parameters and inclusion patterns matter\n\n- APIs with domain-specific conventions not captured in schemas\n\n- Similar tools where examples clarify which one to use (e.g., create_ticket vs create_incident )\n\nLess beneficial when:\n\n- Simple single-parameter tools with obvious usage\n\n- Standard formats like URLs or emails that Claude already understands\n\n- Validation concerns better handled by JSON Schema constraints\n\nBest practices\n\nBuilding agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.\n\nLayer features strategically\n\nNot every agent needs to use all three features for a given task. Start with your biggest bottleneck:\n\n- Context bloat from tool definitions Tool Search Tool\n\n- Large intermediate results polluting context Programmatic Tool Calling\n\n- Parameter errors and malformed calls Tool Use Examples\n\nThis focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.\n\nThen layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.\n\nSet up Tool Search Tool for better discovery\n\nTool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.\n\n```\n// Good\n{\n    \"name\": \"search_customer_orders\",\n    \"description\": \"Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info.\"\n}\n\n// Bad\n{\n    \"name\": \"query_db_orders\",\n    \"description\": \"Execute order query\"\n}\n```\n\nAdd system prompt guidance so Claude knows what's available:\n\n```\nYou have access to tools for Slack messaging, Google Drive file management, \nJira ticket tracking, and GitHub repository operations. Use the tool search \nto find specific capabilities.\n```\n\nKeep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.\n\nSet up Programmatic Tool Calling for correct execution\n\nSince Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:\n\n```\n{\n    \"name\": \"get_orders\",\n    \"description\": \"Retrieve orders for a customer.\nReturns:\n    List of order objects, each containing:\n    - id (str): Order identifier\n    - total (float): Order total in USD\n    - status (str): One of 'pending', 'shipped', 'delivered'\n    - items (list): Array of {sku, quantity, price}\n    - created_at (str): ISO 8601 timestamp\"\n}\n```\n\nSee below for opt-in tools that benefit from programmatic orchestration:\n\n- Tools that can run in parallel (independent operations)\n\n- Operations safe to retry (idempotent)\n\nSet up Tool Use Examples for parameter accuracy\n\nCraft examples for behavioral clarity:\n\n- Use realistic data (real city names, plausible prices, not \"string\" or \"value\")\n\n- Show variety with minimal, partial, and full specification patterns\n\n- Keep it concise: 1-5 examples per tool\n\n- Focus on ambiguity (only add examples where correct usage isn't obvious from schema)\n\nGetting started\n\nThese features are available in beta. To enable them, add the beta header and include the tools you need:\n\n```\nclient.beta.messages.create(\n    betas=[\"advanced-tool-use-2025-11-20\"],\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=4096,\n    tools=[\n        {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n        {\"type\": \"code_execution_20250825\", \"name\": \"code_execution\"},\n        # Your tools with defer_loading, allowed_callers, and input_examples\n    ]\n)\n```\n\nFor detailed API documentation and SDK examples, see our:\n\n- D ocumentation and cookbook for Tool Search Tool\n\n- Documentation and cookbook for Programmatic Tool Calling\n\n- Documentation for Tool Use Examples\n\nThese features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.\n\nWe're excited to see what you build.\n\nAcknowledgements\n\nWritten by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM , Cloudflare's Code Mode and Code Execution as MCP . Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.",
      "images": [
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/151600be7f9c23247aad8dcb6aacb2e1ab024f44-1000x1000.svg",
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/f359296f770706608901eadaffbff4ca0b67874c-1999x1125.png",
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png"
      ]
    },
    "translations": {
      "es": {
        "title": "Nuevas funciones de uso avanzado de herramientas en Claude",
        "content": "Anthropic lanzó Tool Search Tool, llamadas programáticas y ejemplos para que Claude descubra y use herramientas según las necesite. El artículo explica que la búsqueda de herramientas reduce el contexto inicial, las llamadas programáticas mueven la orquestación a código y los ejemplos enseñan patrones correctos. Con estas funciones, Claude puede manejar bibliotecas enormes con menos tokens y elegir entre lenguaje natural o código para ejecutar tareas.",
        "edited": false
      },
      "ukr": {
        "title": "Розширений контроль над інструментами у Claude",
        "content": "Anthropic додала Tool Search Tool, програмні виклики та приклади використання, щоб Claude могла шукати і застосовувати інструменти за потребою. У статті показано, що пошук інструментів скорочує контекст, програмні виклики переносять логіку в код, а приклади допомагають уникати помилкових параметрів. Це зменшує витрати токенів і дає змогу перемикатися між кодом та природною мовою під час роботи.",
        "edited": false
      }
    },
    "metadata": {
      "tags": [
        "Anthropic",
        "MCP",
        "Tooling"
      ]
    }
  },
  {
    "id": "sample-anthropic-code-execution",
    "source": "Anthropic",
    "source_url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
    "title": "Code execution with MCP: Building more efficient agents",
    "author": "Anthropic Engineering",
    "published_date": "2025-11-04T00:00:00Z",
    "scraped_at": 1733203200000,
    "status": "published",
    "content": {
      "original_html": "",
      "text": "The Model Context Protocol (MCP) is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol-developers implement MCP once in their agent and it unlocks an entire ecosystem of integrations.\n\nSince launching MCP in November 2024, adoption has been rapid: the community has built thousands of MCP servers , SDKs are available for all major programming languages, and the industry has adopted MCP as the de-facto standard for connecting agents to tools and data.\n\nToday developers routinely build agents with access to hundreds or thousands of tools across dozens of MCP servers. However, as the number of connected tools grows, loading all tool definitions upfront and passing intermediate results through the context window slows down agents and increases costs.\n\nIn this blog we'll explore how code execution can enable agents to interact with MCP servers more efficiently, handling more tools while using fewer tokens.\n\nExcessive token consumption from tools makes agents less efficient\n\nAs MCP usage scales, there are two common patterns that can increase agent cost and latency:\n\n- Tool definitions overload the context window;\n\n- Intermediate tool results consume additional tokens.\n\n1. Tool definitions overload the context window\n\nMost MCP clients load all tool definitions upfront directly into context, exposing them to the model using a direct tool-calling syntax. These tool definitions might look like:\n\n```\ngdrive.getDocument\n     Description: Retrieves a document from Google Drive\n     Parameters:\n                documentId (required, string): The ID of the document to retrieve\n                fields (optional, string): Specific fields to return\n     Returns: Document object with title, body content, metadata, permissions, etc.\n```\n\n```\nsalesforce.updateRecord\n    Description: Updates a record in Salesforce\n    Parameters:\n               objectType (required, string): Type of Salesforce object (Lead, Contact,      Account, etc.)\n               recordId (required, string): The ID of the record to update\n               data (required, object): Fields to update with their new values\n     Returns: Updated record object with confirmation\n```\n\nTool descriptions occupy more context window space, increasing response time and costs. In cases where agents are connected to thousands of tools, they'll need to process hundreds of thousands of tokens before reading a request.\n\n2. Intermediate tool results consume additional tokens\n\nMost MCP clients allow models to directly call MCP tools. For example, you might ask your agent: \"Download my meeting transcript from Google Drive and attach it to the Salesforce lead.\"\n\nThe model will make calls like:\n\n```\nTOOL CALL: gdrive.getDocument(documentId: \"abc123\")\n        → returns \"Discussed Q4 goals...\\n[full transcript text]\"\n           (loaded into model context)\n\nTOOL CALL: salesforce.updateRecord(\n\t\t\tobjectType: \"SalesMeeting\",\n\t\t\trecordId: \"00Q5f000001abcXYZ\",\n  \t\t\tdata: { \"Notes\": \"Discussed Q4 goals...\\n[full transcript text written out]\" }\n\t\t)\n\t\t(model needs to write entire transcript into context again)\n```\n\nEvery intermediate result must pass through the model. In this example, the full call transcript flows through twice. For a 2-hour sales meeting, that could mean processing an additional 50,000 tokens. Even larger documents may exceed context window limits, breaking the workflow.\n\nWith large documents or complex data structures, models may be more likely to make mistakes when copying data between tool calls.\n\n[[IMAGE_1|The MCP client loads tool definitions into the model's context window and orchestrates a message loop where each tool call and result passes through the model between operations.]]\n\nCode execution with MCP improves context efficiency\n\nWith code execution environments becoming more common for agents, a solution is to present MCP servers as code APIs rather than direct tool calls. The agent can then write code to interact with MCP servers. This approach addresses both challenges: agents can load only the tools they need and process data in the execution environment before passing results back to the model.\n\nThere are a number of ways to do this. One approach is to generate a file tree of all available tools from connected MCP servers. Here's an implementation using TypeScript:\n\n```\nservers\n├── google-drive\n│   ├── getDocument.ts\n│   ├── ... (other tools)\n│   └── index.ts\n├── salesforce\n│   ├── updateRecord.ts\n│   ├── ... (other tools)\n│   └── index.ts\n└── ... (other servers)\n```\n\nThen each tool corresponds to a file, something like:\n\n```\n// ./servers/google-drive/getDocument.ts\nimport { callMCPTool } from \"../../../client.js\";\n\ninterface GetDocumentInput {\n  documentId: string;\n}\n\ninterface GetDocumentResponse {\n  content: string;\n}\n\n/* Read a document from Google Drive */\nexport async function getDocument(input: GetDocumentInput): Promise<GetDocumentResponse> {\n  return callMCPTool<GetDocumentResponse>('google_drive__get_document', input);\n}\n```\n\nOur Google Drive to Salesforce example above becomes the code:\n\n```\n// Read transcript from Google Docs and add to Salesforce prospect\nimport * as gdrive from './servers/google-drive';\nimport * as salesforce from './servers/salesforce';\n\nconst transcript = (await gdrive.getDocument({ documentId: 'abc123' })).content;\nawait salesforce.updateRecord({\n  objectType: 'SalesMeeting',\n  recordId: '00Q5f000001abcXYZ',\n  data: { Notes: transcript }\n});\n```\n\nThe agent discovers tools by exploring the filesystem: listing the ./servers/ directory to find available servers (like google-drive and salesforce ), then reading the specific tool files it needs (like getDocument.ts and updateRecord.ts ) to understand each tool's interface. This lets the agent load only the definitions it needs for the current task. This reduces the token usage from 150,000 tokens to 2,000 tokens-a time and cost saving of 98.7% .\n\nCloudflare published similar findings , referring to code execution with MCP as \"Code Mode.\" The core insight is the same: LLMs are adept at writing code and developers should take advantage of this strength to build agents that interact with MCP servers more efficiently.\n\nBenefits of code execution with MCP\n\nCode execution with MCP enables agents to use context more efficiently by loading tools on demand, filtering data before it reaches the model, and executing complex logic in a single step. There are also security and state management benefits to using this approach.\n\nProgressive disclosure\n\nModels are great at navigating filesystems. Presenting tools as code on a filesystem allows models to read tool definitions on-demand, rather than reading them all up-front.\n\nAlternatively, a search_tools tool can be added to the server to find relevant definitions. For example, when working with the hypothetical Salesforce server used above, the agent searches for \"salesforce\" and loads only those tools that it needs for the current task. Including a detail level parameter in the search_tools tool that allows the agent to select the level of detail required (such as name only, name and description, or the full definition with schemas) also helps the agent conserve context and find tools efficiently.\n\nContext efficient tool results\n\nWhen working with large datasets, agents can filter and transform results in code before returning them. Consider fetching a 10,000-row spreadsheet:\n\n```\n// Without code execution - all rows flow through context\nTOOL CALL: gdrive.getSheet(sheetId: 'abc123')\n        → returns 10,000 rows in context to filter manually\n\n// With code execution - filter in the execution environment\nconst allRows = await gdrive.getSheet({ sheetId: 'abc123' });\nconst pendingOrders = allRows.filter(row => \n  row[\"Status\"] === 'pending'\n);\nconsole.log(`Found ${pendingOrders.length} pending orders`);\nconsole.log(pendingOrders.slice(0, 5)); // Only log first 5 for review\n```\n\nThe agent sees five rows instead of 10,000. Similar patterns work for aggregations, joins across multiple data sources, or extracting specific fields-all without bloating the context window.\n\nMore powerful and context-efficient control flow\n\nLoops, conditionals, and error handling can be done with familiar code patterns rather than chaining individual tool calls. For example, if you need a deployment notification in Slack, the agent can write:\n\n```\nlet found = false;\nwhile (!found) {\n  const messages = await slack.getChannelHistory({ channel: 'C123456' });\n  found = messages.some(m => m.text.includes('deployment complete'));\n  if (!found) await new Promise(r => setTimeout(r, 5000));\n}\nconsole.log('Deployment notification received');\n```\n\nThis approach is more efficient than alternating between MCP tool calls and sleep commands through the agent loop.\n\nAdditionally, being able to write out a conditional tree that gets executed also saves on \"time to first token\" latency: rather than having to wait for a model to evaluate an if-statement, the agent can let the code execution environment do this.\n\nPrivacy-preserving operations\n\nWhen agents use code execution with MCP, intermediate results stay in the execution environment by default. This way, the agent only sees what you explicitly log or return, meaning data you don't wish to share with the model can flow through your workflow without ever entering the model's context.\n\nFor even more sensitive workloads, the agent harness can tokenize sensitive data automatically. For example, imagine you need to import customer contact details from a spreadsheet into Salesforce. The agent writes:\n\n```\nconst sheet = await gdrive.getSheet({ sheetId: 'abc123' });\nfor (const row of sheet.rows) {\n  await salesforce.updateRecord({\n    objectType: 'Lead',\n    recordId: row.salesforceId,\n    data: { \n      Email: row.email,\n      Phone: row.phone,\n      Name: row.name\n    }\n  });\n}\nconsole.log(`Updated ${sheet.rows.length} leads`);\n```\n\nThe MCP client intercepts the data and tokenizes PII before it reaches the model:\n\n```\n// What the agent would see, if it logged the sheet.rows:\n[\n  { salesforceId: '00Q...', email: '[EMAIL_1]', phone: '[PHONE_1]', name: '[NAME_1]' },\n  { salesforceId: '00Q...', email: '[EMAIL_2]', phone: '[PHONE_2]', name: '[NAME_2]' },\n  ...\n]\n```\n\nThen, when the data is shared in another MCP tool call, it is untokenized via a lookup in the MCP client. The real email addresses, phone numbers, and names flow from Google Sheets to Salesforce, but never through the model. This prevents the agent from accidentally logging or processing sensitive data. You can also use this to define deterministic security rules, choosing where data can flow to and from.\n\nState persistence and skills\n\nCode execution with filesystem access allows agents to maintain state across operations. Agents can write intermediate results to files, enabling them to resume work and track progress:\n\n```\nconst leads = await salesforce.query({ \n  query: 'SELECT Id, Email FROM Lead LIMIT 1000' \n});\nconst csvData = leads.map(l => `${l.Id},${l.Email}`).join('\\n');\nawait fs.writeFile('./workspace/leads.csv', csvData);\n\n// Later execution picks up where it left off\nconst saved = await fs.readFile('./workspace/leads.csv', 'utf-8');\n```\n\nAgents can also persist their own code as reusable functions. Once an agent develops working code for a task, it can save that implementation for future use:\n\n```\n// In ./skills/save-sheet-as-csv.ts\nimport * as gdrive from './servers/google-drive';\nexport async function saveSheetAsCsv(sheetId: string) {\n  const data = await gdrive.getSheet({ sheetId });\n  const csv = data.map(row => row.join(',')).join('\\n');\n  await fs.writeFile(`./workspace/sheet-${sheetId}.csv`, csv);\n  return `./workspace/sheet-${sheetId}.csv`;\n}\n\n// Later, in any agent execution:\nimport { saveSheetAsCsv } from './skills/save-sheet-as-csv';\nconst csvPath = await saveSheetAsCsv('abc123');\n```\n\nThis ties in closely to the concept of Skills , folders of reusable instructions, scripts, and resources for models to improve performance on specialized tasks. Adding a SKILL.md file to these saved functions creates a structured skill that models can reference and use. Over time, this allows your agent to build a toolbox of higher-level capabilities, evolving the scaffolding that it needs to work most effectively.\n\nNote that code execution introduces its own complexity. Running agent-generated code requires a secure execution environment with appropriate sandboxing , resource limits, and monitoring. These infrastructure requirements add operational overhead and security considerations that direct tool calls avoid. The benefits of code execution-reduced token costs, lower latency, and improved tool composition-should be weighed against these implementation costs.\n\nSummary\n\nMCP provides a foundational protocol for agents to connect to many tools and systems. However, once too many servers are connected, tool definitions and results can consume excessive tokens, reducing agent efficiency.\n\nAlthough many of the problems here feel novel-context management, tool composition, state persistence-they have known solutions from software engineering. Code execution applies these established patterns to agents, letting them use familiar programming constructs to interact with MCP servers more efficiently. If you implement this approach, we encourage you to share your findings with the MCP community .\n\nAcknowledgments\n\nThis article was written by Adam Jones and Conor Kelly. Thanks to Jeremy Fox, Jerome Swannack, Stuart Ritchie, Molly Vorwerck, Matt Samuels, and Maggie Vo for feedback on drafts of this post.",
      "images": [
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/42f40f6fae9ec2d7cf2e5a98908a16d0216b91be-1000x1000.svg",
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/9ecf165020005c09a22a9472cee6309555485619-1920x1080.png"
      ]
    },
    "translations": {
      "es": {
        "title": "Ejecución de código con MCP para agentes más eficientes",
        "content": "Anthropic explica cómo combinar MCP con ejecución de código para reducir tokens y latencia. Claude descubre herramientas bajo demanda, filtra datos grandes fuera del modelo y solo devuelve la salida final. El enfoque incluye búsqueda diferida, orquestación paralela, ejemplos de uso para evitar errores y manejo de PII fuera del contexto, logrando flujos más precisos y ligeros.",
        "edited": false
      },
      "ukr": {
        "title": "Виконання коду з MCP для економніших агентів",
        "content": "Anthropic показує, як поєднати MCP з виконанням коду, щоб не перенавантажувати контекст. Claude шукає інструменти за потребою, обробляє великі дані поза моделлю й повертає лише фінальний результат. Відкладене завантаження, паралельні виклики, приклади для параметрів і робота з PII поза контекстом зменшують токени й затримки та підвищують точність.",
        "edited": false
      }
    },
    "metadata": {
      "tags": [
        "Anthropic",
        "MCP",
        "Engineering"
      ]
    }
  }
]
