[
  {
    "id": "openai-gpt-5-2-for-science-and-math",
    "source": "OpenAI",
    "source_url": "https://openai.com/index/gpt-5-2-for-science-and-math/",
    "title": "Advancing science and math with GPT-5.2",
    "author": "OpenAI",
    "published_date": "2025-12-11T00:00:00Z",
    "scraped_at": 1765646993208,
    "status": "published",
    "content": {
      "original_html": "",
      "text": "Quick read\n- GPT-5.2 Pro and GPT-5.2 Thinking target math and science work, with top scores on GPQA Diamond and FrontierMath.\n- Emphasis on precise, multi-step reasoning for reliable scientific workflows like coding, data analysis, and experimental design.\n- Case study: GPT-5.2 Pro produced a proof resolving an open COLT learning-curve monotonicity problem for maximum likelihood estimators; humans verified and extended the result.\n- Outlook: frontier models can accelerate research exploration, but human judgment and verification remain essential.\n\nGPT-5.2 is positioned as OpenAI's strongest model yet for scientific and mathematical tasks. The release highlights improvements in precise multi-step reasoning that reduce errors across simulations, statistics, forecasting, and modeling. OpenAI argues these advances represent broader, transferable reasoning ability relevant to AGI.\n\nOn GPQA Diamond, a graduate-level science benchmark without tool use, GPT-5.2 Pro scores 93.2% and GPT-5.2 Thinking 92.4%. On FrontierMath (Tier 1\u20133), GPT-5.2 Thinking sets a new state of the art, solving 40.3% of expert-level math problems with code tools enabled.\n\nA detailed case study describes how GPT-5.2 Pro solved an open problem about whether learning curves for maximum likelihood estimators stay monotonic as more data is added. The model produced a full proof, which human experts verified and extended to higher-dimensional settings and other statistical models. Humans focused on validation and clear exposition rather than supplying proof scaffolding.\n\nThe article frames GPT-5.2 as a research assistant that can propose detailed arguments and hypotheses, while humans retain responsibility for checking assumptions, correctness, and domain context. The piece suggests workflows that combine model-generated reasoning with rigorous human oversight to accelerate theoretical and scientific progress.",
      "images": [
        "https://images.ctfassets.net/kftzwdyauwt9/1Tc0kBv3H0HRXf7SsizFfA/1a8b076fa71453ac7e822e30756fc066/5.2-Blog-1x1.png?w=3840&q=90&fm=webp"
      ]
    },
    "translations": {
      "es": {
        "title": "GPT-5.2 para ciencia y matemáticas",
        "content": "Lectura rápida\n- GPT-5.2 Pro y GPT-5.2 Thinking están orientados al trabajo en matemáticas y ciencia, con puntuaciones líderes en GPQA Diamond y FrontierMath.\n- El enfoque está en el razonamiento preciso y de múltiples pasos para flujos de trabajo científicos fiables como programación, análisis de datos y diseño experimental.\n- Caso de estudio: GPT-5.2 Pro generó una prueba que resolvió un problema abierto sobre la monotonía de las curvas de aprendizaje en estimadores de máxima verosimilitud; expertos humanos verificaron y ampliaron el resultado.\n- Perspectiva: los modelos de frontera pueden acelerar la exploración científica, pero el juicio y la verificación humana siguen siendo esenciales.\n\nGPT-5.2 se presenta como el modelo más sólido de OpenAI hasta la fecha para tareas científicas y matemáticas. El lanzamiento destaca mejoras en el razonamiento preciso de múltiples pasos que reducen errores en simulaciones, estadística, predicción y modelado. OpenAI sostiene que estos avances reflejan una capacidad de razonamiento más amplia y transferible, relevante para el progreso hacia la AGI.\n\nEn GPQA Diamond, un benchmark de ciencia a nivel de posgrado sin uso de herramientas, GPT-5.2 Pro alcanza un 93.2% y GPT-5.2 Thinking un 92.4%. En FrontierMath (niveles 1–3), GPT-5.2 Thinking establece un nuevo estado del arte al resolver el 40.3% de los problemas matemáticos de nivel experto con herramientas de código habilitadas.\n\nUn caso de estudio detallado describe cómo GPT-5.2 Pro resolvió un problema abierto sobre si las curvas de aprendizaje de los estimadores de máxima verosimilitud permanecen monótonas al añadir más datos. El modelo produjo una demostración completa, que expertos humanos verificaron y extendieron a configuraciones de mayor dimensión y otros modelos estadísticos. Los humanos se centraron en la validación y la claridad de la exposición, más que en proporcionar la estructura de la prueba.\n\nEl artículo presenta a GPT-5.2 como un asistente de investigación capaz de proponer argumentos e hipótesis detalladas, mientras que los humanos mantienen la responsabilidad de comprobar supuestos, corrección y contexto del dominio. Se proponen flujos de trabajo que combinan el razonamiento generado por el modelo con una supervisión humana rigurosa para acelerar el progreso teórico y científico.",
        "edited": false
      },
      "ukr": {
        "title": "GPT-5.2 для науки та математики",
        "content": "Коротко\n- GPT-5.2 Pro та GPT-5.2 Thinking орієнтовані на математичні й наукові задачі та демонструють провідні результати на GPQA Diamond і FrontierMath.\n- Основний акцент зроблено на точному багатокроковому міркуванні для надійних наукових workflow, зокрема програмування, аналізу даних і експериментального дизайну.\n- Кейс: GPT-5.2 Pro побудував доведення, яке розв’язало відкрите питання про монотонність кривих навчання для оцінок максимальної правдоподібності; люди перевірили й розширили результат.\n- Перспектива: frontier-моделі можуть прискорювати дослідницький пошук, але людське судження та перевірка залишаються критично важливими.\n\nGPT-5.2 позиціонується як найсильніша модель OpenAI для наукових і математичних задач на сьогодні. У релізі підкреслюються покращення в точному багатокроковому міркуванні, що зменшує кількість помилок у симуляціях, статистиці, прогнозуванні та моделюванні. OpenAI вважає, що ці досягнення свідчать про ширшу та переносну здатність до міркування, релевантну для прогресу до AGI.\n\nНа GPQA Diamond — науковому бенчмарку рівня магістратури та PhD без використання інструментів — GPT-5.2 Pro набирає 93.2%, а GPT-5.2 Thinking — 92.4%. На FrontierMath (рівні 1–3) GPT-5.2 Thinking встановлює новий state of the art, розв’язуючи 40.3% математичних задач експертного рівня з увімкненими код-інструментами.\n\nДетальний кейс описує, як GPT-5.2 Pro розв’язав відкрите питання про те, чи залишаються криві навчання для оцінок максимальної правдоподібності монотонними зі збільшенням обсягу даних. Модель згенерувала повне доведення, яке експерти-люди перевірили та узагальнили на багатовимірні випадки й інші статистичні моделі. Люди зосередилися на перевірці коректності та чіткому викладі, а не на побудові каркасу доведення.\n\nСтаття подає GPT-5.2 як дослідницького асистента, здатного пропонувати детальні аргументи й гіпотези, водночас залишаючи за людьми відповідальність за перевірку припущень, коректності та контексту предметної області. Пропонуються workflow, що поєднують міркування, згенеровані моделлю, з суворим людським контролем для прискорення теоретичного й наукового прогресу.",
        "edited": false
      }
    },
    "metadata": {
      "tags": [
        "OpenAI",
        "GPT-5.2",
        "Science",
        "Math"
      ]
    }
  },
  {
    "id": "sample-anthropic-advanced-tool-use",
    "source": "Anthropic",
    "source_url": "https://www.anthropic.com/engineering/advanced-tool-use",
    "title": "Introducing advanced tool use on the Claude Developer Platform",
    "author": "Anthropic Engineering",
    "published_date": "2025-11-24T00:00:00Z",
    "scraped_at": 1733203200000,
    "status": "published",
    "content": {
      "original_html": "",
      "text": "The future of AI agents is one where models work seamlessly across hundreds or thousands of tools. An IDE assistant that integrates git operations, file manipulation, package managers, testing frameworks, and deployment pipelines. An operations coordinator that connects Slack, GitHub, Google Drive, Jira, company databases, and dozens of MCP servers simultaneously.\n\nTo build effective agents , they need to work with unlimited tool libraries without stuffing every definition into context upfront. Our blog article on using code execution with MCP discussed how tool results and definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what's relevant for the current task.\n\nAgents also need the ability to call tools from code. When using natural language tool calling, each invocation requires a full inference pass, and intermediate results pile up in context whether they're useful or not. Code is a natural fit for orchestration logic, such as loops, conditionals, and data transformations. Agents need the flexibility to choose between code execution and inference based on the task at hand.\n\nAgents also need to learn correct tool usage from examples, not just schema definitions. JSON schemas define what's structurally valid, but can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.\n\nToday, we're releasing three features that make this possible:\n\n- Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window\n\n- Programmatic Tool Calling , which allows Claude to invoke tools in a code execution environment reducing the impact on the model's context window\n\n- Tool Use Examples , which provides a universal standard for demonstrating how to effectively use a given tool\n\nIn internal testing, we've found these features have helped us build things that wouldn't have been possible with conventional tool use patterns. For example, Claude for Excel uses Programmatic Tool Calling to read and modify spreadsheets with thousands of rows without overloading the model's context window.\n\nBased on our experience, we believe these features open up new possibilities for what you can build with Claude.\n\nTool Search Tool\n\nThe challenge\n\nMCP tool definitions provide important context, but as more servers connect, those tokens can add up. Consider a five-server setup:\n\n- GitHub: 35 tools (~26K tokens)\n\n- Slack: 11 tools (~21K tokens)\n\n- Sentry: 5 tools (~3K tokens)\n\n- Grafana: 5 tools (~3K tokens)\n\n- Splunk: 2 tools (~2K tokens)\n\nThat's 58 tools consuming approximately 55K tokens before the conversation even starts. Add more servers like Jira (which alone uses ~17K tokens) and you're quickly approaching 100K+ token overhead. At Anthropic, we've seen tool definitions consume 134K tokens before optimization.\n\nBut token cost isn't the only issue. The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names like notification-send-user vs. notification-send-channel .\n\nOur solution\n\nInstead of loading all tool definitions upfront, the Tool Search Tool discovers tools on-demand. Claude only sees the tools it actually needs for the current task.\n\n[[IMAGE_1|Tool Search Tool preserves 191,300 tokens of context compared to 122,800 with Claude's traditional approach.]]\n\nTraditional approach:\n\n- All tool definitions loaded upfront (~72K tokens for 50+ MCP tools)\n\n- Conversation history and system prompt compete for remaining space\n\n- Total context consumption: ~77K tokens before any work begins\n\nWith the Tool Search Tool:\n\n- Only the Tool Search Tool loaded upfront (~500 tokens)\n\n- Tools discovered on-demand as needed (3-5 relevant tools, ~3K tokens)\n\n- Total context consumption: ~8.7K tokens, preserving 95% of context window\n\nThis represents an 85% reduction in token usage while maintaining access to your full tool library. Internal testing showed significant accuracy improvements on MCP evaluations when working with large tool libraries. Opus 4 improved from 49% to 74%, and Opus 4.5 improved from 79.5% to 88.1% with Tool Search Tool enabled.\n\nHow the Tool Search Tool works\n\nThe Tool Search Tool lets Claude dynamically discover tools instead of loading all definitions upfront. You provide all your tool definitions to the API, but mark tools with defer_loading: true to make them discoverable on-demand. Deferred tools aren't loaded into Claude's context initially. Claude only sees the Tool Search Tool itself plus any tools with defer_loading: false (your most critical, frequently-used tools).\n\nWhen Claude needs specific capabilities, it searches for relevant tools. The Tool Search Tool returns references to matching tools, which get expanded into full definitions in Claude's context.\n\nFor example, if Claude needs to interact with GitHub, it searches for \"github,\" and only github.createPullRequest and github.listIssues get loaded-not your other 50+ tools from Slack, Jira, and Google Drive.\n\nThis way, Claude has access to your full tool library while only paying the token cost for tools it actually needs.\n\nPrompt caching note: Tool Search Tool doesn't break prompt caching because deferred tools are excluded from the initial prompt entirely. They're only added to context after Claude searches for them, so your system prompt and core tool definitions remain cacheable.\n\nImplementation:\n\n```\n{\n  \"tools\": [\n    // Include a tool search tool (regex, BM25, or custom)\n    {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n\n    // Mark tools for on-demand discovery\n    {\n      \"name\": \"github.createPullRequest\",\n      \"description\": \"Create a pull request\",\n      \"input_schema\": {...},\n      \"defer_loading\": true\n    }\n    // ... hundreds more deferred tools with defer_loading: true\n  ]\n}\n```\n\nFor MCP servers, you can defer loading entire servers while keeping specific high-use tools loaded:\n\n```\n{\n  \"type\": \"mcp_toolset\",\n  \"mcp_server_name\": \"google-drive\",\n  \"default_config\": {\"defer_loading\": true}, # defer loading the entire server\n  \"configs\": {\n    \"search_files\": {\n\"defer_loading\": false\n    }  // Keep most used tool loaded\n  }\n}\n```\n\nThe Claude Developer Platform provides regex-based and BM25-based search tools out of the box, but you can also implement custom search tools using embeddings or other strategies.\n\nWhen to use the Tool Search Tool\n\nLike any architectural decision, enabling the Tool Search Tool involves trade-offs. The feature adds a search step before tool invocation, so it delivers the best ROI when the context savings and accuracy improvements outweigh additional latency.\n\nUse it when:\n\n- Tool definitions consuming >10K tokens\n\n- Experiencing tool selection accuracy issues\n\n- Building MCP-powered systems with multiple servers\n\n- 10+ tools available\n\nLess beneficial when:\n\n- Small tool library (<10 tools)\n\n- All tools used frequently in every session\n\n- Tool definitions are compact\n\nProgrammatic Tool Calling\n\nThe challenge\n\nTraditional tool calling creates two fundamental problems as workflows become more complex:\n\n- Context pollution from intermediate results : When Claude analyzes a 10MB log file for error patterns, the entire file enters its context window, even though Claude only needs a summary of error frequencies. When fetching customer data across multiple tables, every record accumulates in context regardless of relevance. These intermediate results consume massive token budgets and can push important information out of the context window entirely.\n\n- Inference overhead and manual synthesis : Each tool call requires a full model inference pass. After receiving results, Claude must \"eyeball\" the data to extract relevant information, reason about how pieces fit together, and decide what to do next-all through natural language processing. A five tool workflow means five inference passes plus Claude parsing each result, comparing values, and synthesizing conclusions. This is both slow and error-prone.\n\nOur solution\n\nProgrammatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips. Instead of Claude requesting tools one at a time with each result being returned to its context, Claude writes code that calls multiple tools, processes their outputs, and controls what information actually enters its context window.\n\nClaude excels at writing code and by letting it express orchestration logic in Python rather than through natural language tool invocations, you get more reliable, precise control flow. Loops, conditionals, data transformations, and error handling are all explicit in code rather than implicit in Claude's reasoning.\n\nExample: Budget compliance check\n\nConsider a common business task: \"Which team members exceeded their Q3 travel budget?\"\n\nYou have three tools available:\n\n- get_team_members(department) - Returns team member list with IDs and levels\n\n- get_expenses(user_id, quarter) - Returns expense line items for a user\n\n- get_budget_by_level(level) - Returns budget limits for an employee level\n\nTraditional approach :\n\n- Fetch team members 20 people\n\n- For each person, fetch their Q3 expenses 20 tool calls, each returning 50-100 line items (flights, hotels, meals, receipts)\n\n- Fetch budget limits by employee level\n\n- All of this enters Claude's context: 2,000+ expense line items (50 KB+)\n\n- Claude manually sums each person's expenses, looks up their budget, compares expenses against budget limits\n\n- More round-trips to the model, significant context consumption\n\nWith Programmatic Tool Calling :\n\nInstead of each tool result returning to Claude, Claude writes a Python script that orchestrates the entire workflow. The script runs in the Code Execution tool (a sandboxed environment), pausing when it needs results from your tools. When you return tool results via the API, they're processed by the script rather than consumed by the model. The script continues executing, and Claude only sees the final output.\n\n[[IMAGE_2|Programmatic Tool Calling enables Claude to orchestrate tools through code rather than through individual API round-trips, allowing for parallel tool execution.]]\n\nHere's what Claude's orchestration code looks like for the budget compliance task:\n\n```\nteam = await get_team_members(\"engineering\")\n\n# Fetch budgets for each unique level\nlevels = list(set(m[\"level\"] for m in team))\nbudget_results = await asyncio.gather(*[\n    get_budget_by_level(level) for level in levels\n])\n\n# Create a lookup dictionary: {\"junior\": budget1, \"senior\": budget2, ...}\nbudgets = {level: budget for level, budget in zip(levels, budget_results)}\n\n# Fetch all expenses in parallel\nexpenses = await asyncio.gather(*[\n    get_expenses(m[\"id\"], \"Q3\") for m in team\n])\n\n# Find employees who exceeded their travel budget\nexceeded = []\nfor member, exp in zip(team, expenses):\n    budget = budgets[member[\"level\"]]\n    total = sum(e[\"amount\"] for e in exp)\n    if total > budget[\"travel_limit\"]:\n        exceeded.append({\n            \"name\": member[\"name\"],\n            \"spent\": total,\n            \"limit\": budget[\"travel_limit\"]\n        })\n\nprint(json.dumps(exceeded))\n```\n\nClaude's context receives only the final result: the two to three people who exceeded their budget. The 2,000+ line items, the intermediate sums, and the budget lookups do not affect Claude's context, reducing consumption from 200KB of raw expense data to just 1KB of results.\n\nThe efficiency gains are substantial:\n\n- Token savings : By keeping intermediate results out of Claude's context, PTC dramatically reduces token consumption. Average usage dropped from 43,588 to 27,297 tokens, a 37% reduction on complex research tasks.\n\n- Reduced latency : Each API round-trip requires model inference (hundreds of milliseconds to seconds). When Claude orchestrates 20+ tool calls in a single code block, you eliminate 19+ inference passes. The API handles tool execution without returning to the model each time.\n\n- Improved accuracy : By writing explicit orchestration logic, Claude makes fewer errors than when juggling multiple tool results in natural language. Internal knowledge retrieval improved from 25.6% to 28.5%; GIA benchmarks from 46.5% to 51.2%.\n\nProduction workflows involve messy data, conditional logic, and operations that need to scale. Programmatic Tool Calling lets Claude handle that complexity programmatically while keeping its focus on actionable results rather than raw data processing.\n\nHow Programmatic Tool Calling works\n\n1. Mark tools as callable from code\n\nAdd code_execution to tools, and set allowed_callers to opt-in tools for programmatic execution:\n\n```\n{\n  \"tools\": [\n    {\n      \"type\": \"code_execution_20250825\",\n      \"name\": \"code_execution\"\n    },\n    {\n      \"name\": \"get_team_members\",\n      \"description\": \"Get all members of a department...\",\n      \"input_schema\": {...},\n      \"allowed_callers\": [\"code_execution_20250825\"] # opt-in to programmatic tool calling\n    },\n    {\n      \"name\": \"get_expenses\",\n \t...\n    },\n    {\n      \"name\": \"get_budget_by_level\",\n\t...\n    }\n  ]\n}\n```\n\nThe API converts these tool definitions into Python functions that Claude can call.\n\n2. Claude writes orchestration code\n\nInstead of requesting tools one at a time, Claude generates Python code:\n\n```\n{\n  \"type\": \"server_tool_use\",\n  \"id\": \"srvtoolu_abc\",\n  \"name\": \"code_execution\",\n  \"input\": {\n    \"code\": \"team = get_team_members('engineering')\\n...\" # the code example above\n  }\n}\n```\n\n3. Tools execute without hitting Claude's context\n\nWhen the code calls get_expenses(), you receive a tool request with a caller field:\n\n```\n{\n  \"type\": \"tool_use\",\n  \"id\": \"toolu_xyz\",\n  \"name\": \"get_expenses\",\n  \"input\": {\"user_id\": \"emp_123\", \"quarter\": \"Q3\"},\n  \"caller\": {\n    \"type\": \"code_execution_20250825\",\n    \"tool_id\": \"srvtoolu_abc\"\n  }\n}\n```\n\nYou provide the result, which is processed in the Code Execution environment rather than Claude's context. This request-response cycle repeats for each tool call in the code.\n\n4. Only final output enters context\n\nWhen the code finishes running, only the results of the code are returned to Claude:\n\n```\n{\n  \"type\": \"code_execution_tool_result\",\n  \"tool_use_id\": \"srvtoolu_abc\",\n  \"content\": {\n    \"stdout\": \"[{\\\"name\\\": \\\"Alice\\\", \\\"spent\\\": 12500, \\\"limit\\\": 10000}...]\"\n  }\n}\n```\n\nThis is all Claude sees, not the 2000+ expense line items processed along the way.\n\nWhen to use Programmatic Tool Calling\n\nProgrammatic Tool Calling adds a code execution step to your workflow. This extra overhead pays off when the token savings, latency improvements, and accuracy gains are substantial.\n\nMost beneficial when:\n\n- Processing large datasets where you only need aggregates or summaries\n\n- Running multi-step workflows with three or more dependent tool calls\n\n- Filtering, sorting, or transforming tool results before Claude sees them\n\n- Handling tasks where intermediate data shouldn't influence Claude's reasoning\n\n- Running parallel operations across many items (checking 50 endpoints, for example)\n\nLess beneficial when:\n\n- Making simple single-tool invocations\n\n- Working on tasks where Claude should see and reason about all intermediate results\n\n- Running quick lookups with small responses\n\nTool Use Examples\n\nThe challenge\n\nJSON Schema excels at defining structure-types, required fields, allowed enums-but it can't express usage patterns: when to include optional parameters, which combinations make sense, or what conventions your API expects.\n\nConsider a support ticket API:\n\n```\n{\n  \"name\": \"create_ticket\",\n  \"input_schema\": {\n    \"properties\": {\n      \"title\": {\"type\": \"string\"},\n      \"priority\": {\"enum\": [\"low\", \"medium\", \"high\", \"critical\"]},\n      \"labels\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n      \"reporter\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"id\": {\"type\": \"string\"},\n          \"name\": {\"type\": \"string\"},\n          \"contact\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"email\": {\"type\": \"string\"},\n              \"phone\": {\"type\": \"string\"}\n            }\n          }\n        }\n      },\n      \"due_date\": {\"type\": \"string\"},\n      \"escalation\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"level\": {\"type\": \"integer\"},\n          \"notify_manager\": {\"type\": \"boolean\"},\n          \"sla_hours\": {\"type\": \"integer\"}\n        }\n      }\n    },\n    \"required\": [\"title\"]\n  }\n}\n```\n\nThe schema defines what's valid, but leaves critical questions unanswered:\n\n- Format ambiguity: Should due_date use \"2024-11-06\", \"Nov 6, 2024\", or \"2024-11-06T00:00:00Z\"?\n\n- ID conventions: Is reporter.id a UUID, \"USR-12345\", or just \"12345\"?\n\n- Nested structure usage: When should Claude populate reporter.contact ?\n\n- Parameter correlations: How do escalation.level and escalation.sla_hours relate to priority?\n\nThese ambiguities can lead to malformed tool calls and inconsistent parameter usage.\n\nOur solution\n\nTool Use Examples let you provide sample tool calls directly in your tool definitions. Instead of relying on schema alone, you show Claude concrete usage patterns:\n\n```\n{\n    \"name\": \"create_ticket\",\n    \"input_schema\": { /* same schema as above */ },\n    \"input_examples\": [\n      {\n        \"title\": \"Login page returns 500 error\",\n        \"priority\": \"critical\",\n        \"labels\": [\"bug\", \"authentication\", \"production\"],\n        \"reporter\": {\n          \"id\": \"USR-12345\",\n          \"name\": \"Jane Smith\",\n          \"contact\": {\n            \"email\": \"jane@acme.com\",\n            \"phone\": \"+1-555-0123\"\n          }\n        },\n        \"due_date\": \"2024-11-06\",\n        \"escalation\": {\n          \"level\": 2,\n          \"notify_manager\": true,\n          \"sla_hours\": 4\n        }\n      },\n      {\n        \"title\": \"Add dark mode support\",\n        \"labels\": [\"feature-request\", \"ui\"],\n        \"reporter\": {\n          \"id\": \"USR-67890\",\n          \"name\": \"Alex Chen\"\n        }\n      },\n      {\n        \"title\": \"Update API documentation\"\n      }\n    ]\n  }\n```\n\nFrom these three examples, Claude learns:\n\n- Format conventions : Dates use YYYY-MM-DD, user IDs follow USR-XXXXX, labels use kebab-case\n\n- Nested structure patterns : How to construct the reporter object with its nested contact object\n\n- Optional parameter correlations : Critical bugs have full contact info + escalation with tight SLAs; feature requests have reporter but no contact/escalation; internal tasks have title only\n\nIn our own internal testing, tool use examples improved accuracy from 72% to 90% on complex parameter handling.\n\nWhen to use Tool Use Examples\n\nTool Use Examples add tokens to your tool definitions, so they're most valuable when accuracy improvements outweigh the additional cost.\n\nMost beneficial when:\n\n- Complex nested structures where valid JSON doesn't imply correct usage\n\n- Tools with many optional parameters and inclusion patterns matter\n\n- APIs with domain-specific conventions not captured in schemas\n\n- Similar tools where examples clarify which one to use (e.g., create_ticket vs create_incident )\n\nLess beneficial when:\n\n- Simple single-parameter tools with obvious usage\n\n- Standard formats like URLs or emails that Claude already understands\n\n- Validation concerns better handled by JSON Schema constraints\n\nBest practices\n\nBuilding agents that take real-world actions means handling scale, complexity, and precision simultaneously. These three features work together to solve different bottlenecks in tool use workflows. Here's how to combine them effectively.\n\nLayer features strategically\n\nNot every agent needs to use all three features for a given task. Start with your biggest bottleneck:\n\n- Context bloat from tool definitions Tool Search Tool\n\n- Large intermediate results polluting context Programmatic Tool Calling\n\n- Parameter errors and malformed calls Tool Use Examples\n\nThis focused approach lets you address the specific constraint limiting your agent's performance, rather than adding complexity upfront.\n\nThen layer additional features as needed. They're complementary: Tool Search Tool ensures the right tools are found, Programmatic Tool Calling ensures efficient execution, and Tool Use Examples ensure correct invocation.\n\nSet up Tool Search Tool for better discovery\n\nTool search matches against names and descriptions, so clear, descriptive definitions improve discovery accuracy.\n\n```\n// Good\n{\n    \"name\": \"search_customer_orders\",\n    \"description\": \"Search for customer orders by date range, status, or total amount. Returns order details including items, shipping, and payment info.\"\n}\n\n// Bad\n{\n    \"name\": \"query_db_orders\",\n    \"description\": \"Execute order query\"\n}\n```\n\nAdd system prompt guidance so Claude knows what's available:\n\n```\nYou have access to tools for Slack messaging, Google Drive file management, \nJira ticket tracking, and GitHub repository operations. Use the tool search \nto find specific capabilities.\n```\n\nKeep your three to five most-used tools always loaded, defer the rest. This balances immediate access for common operations with on-demand discovery for everything else.\n\nSet up Programmatic Tool Calling for correct execution\n\nSince Claude writes code to parse tool outputs, document return formats clearly. This helps Claude write correct parsing logic:\n\n```\n{\n    \"name\": \"get_orders\",\n    \"description\": \"Retrieve orders for a customer.\nReturns:\n    List of order objects, each containing:\n    - id (str): Order identifier\n    - total (float): Order total in USD\n    - status (str): One of 'pending', 'shipped', 'delivered'\n    - items (list): Array of {sku, quantity, price}\n    - created_at (str): ISO 8601 timestamp\"\n}\n```\n\nSee below for opt-in tools that benefit from programmatic orchestration:\n\n- Tools that can run in parallel (independent operations)\n\n- Operations safe to retry (idempotent)\n\nSet up Tool Use Examples for parameter accuracy\n\nCraft examples for behavioral clarity:\n\n- Use realistic data (real city names, plausible prices, not \"string\" or \"value\")\n\n- Show variety with minimal, partial, and full specification patterns\n\n- Keep it concise: 1-5 examples per tool\n\n- Focus on ambiguity (only add examples where correct usage isn't obvious from schema)\n\nGetting started\n\nThese features are available in beta. To enable them, add the beta header and include the tools you need:\n\n```\nclient.beta.messages.create(\n    betas=[\"advanced-tool-use-2025-11-20\"],\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=4096,\n    tools=[\n        {\"type\": \"tool_search_tool_regex_20251119\", \"name\": \"tool_search_tool_regex\"},\n        {\"type\": \"code_execution_20250825\", \"name\": \"code_execution\"},\n        # Your tools with defer_loading, allowed_callers, and input_examples\n    ]\n)\n```\n\nFor detailed API documentation and SDK examples, see our:\n\n- D ocumentation and cookbook for Tool Search Tool\n\n- Documentation and cookbook for Programmatic Tool Calling\n\n- Documentation for Tool Use Examples\n\nThese features move tool use from simple function calling toward intelligent orchestration. As agents tackle more complex workflows spanning dozens of tools and large datasets, dynamic discovery, efficient execution, and reliable invocation become foundational.\n\nWe're excited to see what you build.\n\nAcknowledgements\n\nWritten by Bin Wu, with contributions from Adam Jones, Artur Renault, Henry Tay, Jake Noble, Nathan McCandlish, Noah Picard, Sam Jiang, and the Claude Developer Platform team. This work builds on foundational research by Chris Gorgolewski, Daniel Jiang, Jeremy Fox and Mike Lambert. We also drew inspiration from across the AI ecosystem, including Joel Pobar's LLMVM , Cloudflare's Code Mode and Code Execution as MCP . Special thanks to Andy Schumeister, Hamish Kerr, Keir Bradwell, Matt Bleifer and Molly Vorwerck for their support.",
      "images": [
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/151600be7f9c23247aad8dcb6aacb2e1ab024f44-1000x1000.svg",
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/f359296f770706608901eadaffbff4ca0b67874c-1999x1125.png",
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/65737d69a3290ed5c1f3c3b8dc873645a9dcc2eb-1999x1491.png"
      ]
    },
    "translations": {
      "es": {
        "title": "Nuevas funciones de uso avanzado de herramientas en Claude",
        "content": "Anthropic lanz\u00f3 Tool Search Tool, llamadas program\u00e1ticas y ejemplos para que Claude descubra y use herramientas seg\u00fan las necesite. El art\u00edculo explica que la b\u00fasqueda de herramientas reduce el contexto inicial, las llamadas program\u00e1ticas mueven la orquestaci\u00f3n a c\u00f3digo y los ejemplos ense\u00f1an patrones correctos. Con estas funciones, Claude puede manejar bibliotecas enormes con menos tokens y elegir entre lenguaje natural o c\u00f3digo para ejecutar tareas.",
        "edited": false
      },
      "ukr": {
        "title": "\u0420\u043e\u0437\u0448\u0438\u0440\u0435\u043d\u0438\u0439 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c \u043d\u0430\u0434 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0443 Claude",
        "content": "Anthropic \u0434\u043e\u0434\u0430\u043b\u0430 Tool Search Tool, \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043d\u0456 \u0432\u0438\u043a\u043b\u0438\u043a\u0438 \u0442\u0430 \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u0438 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f, \u0449\u043e\u0431 Claude \u043c\u043e\u0433\u043b\u0430 \u0448\u0443\u043a\u0430\u0442\u0438 \u0456 \u0437\u0430\u0441\u0442\u043e\u0441\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0438 \u0437\u0430 \u043f\u043e\u0442\u0440\u0435\u0431\u043e\u044e. \u0423 \u0441\u0442\u0430\u0442\u0442\u0456 \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043e, \u0449\u043e \u043f\u043e\u0448\u0443\u043a \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0456\u0432 \u0441\u043a\u043e\u0440\u043e\u0447\u0443\u0454 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442, \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043d\u0456 \u0432\u0438\u043a\u043b\u0438\u043a\u0438 \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u044f\u0442\u044c \u043b\u043e\u0433\u0456\u043a\u0443 \u0432 \u043a\u043e\u0434, \u0430 \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u0438 \u0434\u043e\u043f\u043e\u043c\u0430\u0433\u0430\u044e\u0442\u044c \u0443\u043d\u0438\u043a\u0430\u0442\u0438 \u043f\u043e\u043c\u0438\u043b\u043a\u043e\u0432\u0438\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0456\u0432. \u0426\u0435 \u0437\u043c\u0435\u043d\u0448\u0443\u0454 \u0432\u0438\u0442\u0440\u0430\u0442\u0438 \u0442\u043e\u043a\u0435\u043d\u0456\u0432 \u0456 \u0434\u0430\u0454 \u0437\u043c\u043e\u0433\u0443 \u043f\u0435\u0440\u0435\u043c\u0438\u043a\u0430\u0442\u0438\u0441\u044f \u043c\u0456\u0436 \u043a\u043e\u0434\u043e\u043c \u0442\u0430 \u043f\u0440\u0438\u0440\u043e\u0434\u043d\u043e\u044e \u043c\u043e\u0432\u043e\u044e \u043f\u0456\u0434 \u0447\u0430\u0441 \u0440\u043e\u0431\u043e\u0442\u0438.",
        "edited": false
      }
    },
    "metadata": {
      "tags": [
        "Anthropic",
        "MCP",
        "Tooling"
      ]
    }
  },
  {
    "id": "sample-anthropic-code-execution",
    "source": "Anthropic",
    "source_url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
    "title": "Code execution with MCP: Building more efficient agents",
    "author": "Anthropic Engineering",
    "published_date": "2025-11-04T00:00:00Z",
    "scraped_at": 1733203200000,
    "status": "published",
    "content": {
      "original_html": "",
      "text": "The Model Context Protocol (MCP) is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol-developers implement MCP once in their agent and it unlocks an entire ecosystem of integrations.\n\nSince launching MCP in November 2024, adoption has been rapid: the community has built thousands of MCP servers , SDKs are available for all major programming languages, and the industry has adopted MCP as the de-facto standard for connecting agents to tools and data.\n\nToday developers routinely build agents with access to hundreds or thousands of tools across dozens of MCP servers. However, as the number of connected tools grows, loading all tool definitions upfront and passing intermediate results through the context window slows down agents and increases costs.\n\nIn this blog we'll explore how code execution can enable agents to interact with MCP servers more efficiently, handling more tools while using fewer tokens.\n\nExcessive token consumption from tools makes agents less efficient\n\nAs MCP usage scales, there are two common patterns that can increase agent cost and latency:\n\n- Tool definitions overload the context window;\n\n- Intermediate tool results consume additional tokens.\n\n1. Tool definitions overload the context window\n\nMost MCP clients load all tool definitions upfront directly into context, exposing them to the model using a direct tool-calling syntax. These tool definitions might look like:\n\n```\ngdrive.getDocument\n     Description: Retrieves a document from Google Drive\n     Parameters:\n                documentId (required, string): The ID of the document to retrieve\n                fields (optional, string): Specific fields to return\n     Returns: Document object with title, body content, metadata, permissions, etc.\n```\n\n```\nsalesforce.updateRecord\n    Description: Updates a record in Salesforce\n    Parameters:\n               objectType (required, string): Type of Salesforce object (Lead, Contact,      Account, etc.)\n               recordId (required, string): The ID of the record to update\n               data (required, object): Fields to update with their new values\n     Returns: Updated record object with confirmation\n```\n\nTool descriptions occupy more context window space, increasing response time and costs. In cases where agents are connected to thousands of tools, they'll need to process hundreds of thousands of tokens before reading a request.\n\n2. Intermediate tool results consume additional tokens\n\nMost MCP clients allow models to directly call MCP tools. For example, you might ask your agent: \"Download my meeting transcript from Google Drive and attach it to the Salesforce lead.\"\n\nThe model will make calls like:\n\n```\nTOOL CALL: gdrive.getDocument(documentId: \"abc123\")\n        \u2192 returns \"Discussed Q4 goals...\\n[full transcript text]\"\n           (loaded into model context)\n\nTOOL CALL: salesforce.updateRecord(\n\t\t\tobjectType: \"SalesMeeting\",\n\t\t\trecordId: \"00Q5f000001abcXYZ\",\n  \t\t\tdata: { \"Notes\": \"Discussed Q4 goals...\\n[full transcript text written out]\" }\n\t\t)\n\t\t(model needs to write entire transcript into context again)\n```\n\nEvery intermediate result must pass through the model. In this example, the full call transcript flows through twice. For a 2-hour sales meeting, that could mean processing an additional 50,000 tokens. Even larger documents may exceed context window limits, breaking the workflow.\n\nWith large documents or complex data structures, models may be more likely to make mistakes when copying data between tool calls.\n\n[[IMAGE_1|The MCP client loads tool definitions into the model's context window and orchestrates a message loop where each tool call and result passes through the model between operations.]]\n\nCode execution with MCP improves context efficiency\n\nWith code execution environments becoming more common for agents, a solution is to present MCP servers as code APIs rather than direct tool calls. The agent can then write code to interact with MCP servers. This approach addresses both challenges: agents can load only the tools they need and process data in the execution environment before passing results back to the model.\n\nThere are a number of ways to do this. One approach is to generate a file tree of all available tools from connected MCP servers. Here's an implementation using TypeScript:\n\n```\nservers\n\u251c\u2500\u2500 google-drive\n\u2502   \u251c\u2500\u2500 getDocument.ts\n\u2502   \u251c\u2500\u2500 ... (other tools)\n\u2502   \u2514\u2500\u2500 index.ts\n\u251c\u2500\u2500 salesforce\n\u2502   \u251c\u2500\u2500 updateRecord.ts\n\u2502   \u251c\u2500\u2500 ... (other tools)\n\u2502   \u2514\u2500\u2500 index.ts\n\u2514\u2500\u2500 ... (other servers)\n```\n\nThen each tool corresponds to a file, something like:\n\n```\n// ./servers/google-drive/getDocument.ts\nimport { callMCPTool } from \"../../../client.js\";\n\ninterface GetDocumentInput {\n  documentId: string;\n}\n\ninterface GetDocumentResponse {\n  content: string;\n}\n\n/* Read a document from Google Drive */\nexport async function getDocument(input: GetDocumentInput): Promise<GetDocumentResponse> {\n  return callMCPTool<GetDocumentResponse>('google_drive__get_document', input);\n}\n```\n\nOur Google Drive to Salesforce example above becomes the code:\n\n```\n// Read transcript from Google Docs and add to Salesforce prospect\nimport * as gdrive from './servers/google-drive';\nimport * as salesforce from './servers/salesforce';\n\nconst transcript = (await gdrive.getDocument({ documentId: 'abc123' })).content;\nawait salesforce.updateRecord({\n  objectType: 'SalesMeeting',\n  recordId: '00Q5f000001abcXYZ',\n  data: { Notes: transcript }\n});\n```\n\nThe agent discovers tools by exploring the filesystem: listing the ./servers/ directory to find available servers (like google-drive and salesforce ), then reading the specific tool files it needs (like getDocument.ts and updateRecord.ts ) to understand each tool's interface. This lets the agent load only the definitions it needs for the current task. This reduces the token usage from 150,000 tokens to 2,000 tokens-a time and cost saving of 98.7% .\n\nCloudflare published similar findings , referring to code execution with MCP as \"Code Mode.\" The core insight is the same: LLMs are adept at writing code and developers should take advantage of this strength to build agents that interact with MCP servers more efficiently.\n\nBenefits of code execution with MCP\n\nCode execution with MCP enables agents to use context more efficiently by loading tools on demand, filtering data before it reaches the model, and executing complex logic in a single step. There are also security and state management benefits to using this approach.\n\nProgressive disclosure\n\nModels are great at navigating filesystems. Presenting tools as code on a filesystem allows models to read tool definitions on-demand, rather than reading them all up-front.\n\nAlternatively, a search_tools tool can be added to the server to find relevant definitions. For example, when working with the hypothetical Salesforce server used above, the agent searches for \"salesforce\" and loads only those tools that it needs for the current task. Including a detail level parameter in the search_tools tool that allows the agent to select the level of detail required (such as name only, name and description, or the full definition with schemas) also helps the agent conserve context and find tools efficiently.\n\nContext efficient tool results\n\nWhen working with large datasets, agents can filter and transform results in code before returning them. Consider fetching a 10,000-row spreadsheet:\n\n```\n// Without code execution - all rows flow through context\nTOOL CALL: gdrive.getSheet(sheetId: 'abc123')\n        \u2192 returns 10,000 rows in context to filter manually\n\n// With code execution - filter in the execution environment\nconst allRows = await gdrive.getSheet({ sheetId: 'abc123' });\nconst pendingOrders = allRows.filter(row => \n  row[\"Status\"] === 'pending'\n);\nconsole.log(`Found ${pendingOrders.length} pending orders`);\nconsole.log(pendingOrders.slice(0, 5)); // Only log first 5 for review\n```\n\nThe agent sees five rows instead of 10,000. Similar patterns work for aggregations, joins across multiple data sources, or extracting specific fields-all without bloating the context window.\n\nMore powerful and context-efficient control flow\n\nLoops, conditionals, and error handling can be done with familiar code patterns rather than chaining individual tool calls. For example, if you need a deployment notification in Slack, the agent can write:\n\n```\nlet found = false;\nwhile (!found) {\n  const messages = await slack.getChannelHistory({ channel: 'C123456' });\n  found = messages.some(m => m.text.includes('deployment complete'));\n  if (!found) await new Promise(r => setTimeout(r, 5000));\n}\nconsole.log('Deployment notification received');\n```\n\nThis approach is more efficient than alternating between MCP tool calls and sleep commands through the agent loop.\n\nAdditionally, being able to write out a conditional tree that gets executed also saves on \"time to first token\" latency: rather than having to wait for a model to evaluate an if-statement, the agent can let the code execution environment do this.\n\nPrivacy-preserving operations\n\nWhen agents use code execution with MCP, intermediate results stay in the execution environment by default. This way, the agent only sees what you explicitly log or return, meaning data you don't wish to share with the model can flow through your workflow without ever entering the model's context.\n\nFor even more sensitive workloads, the agent harness can tokenize sensitive data automatically. For example, imagine you need to import customer contact details from a spreadsheet into Salesforce. The agent writes:\n\n```\nconst sheet = await gdrive.getSheet({ sheetId: 'abc123' });\nfor (const row of sheet.rows) {\n  await salesforce.updateRecord({\n    objectType: 'Lead',\n    recordId: row.salesforceId,\n    data: { \n      Email: row.email,\n      Phone: row.phone,\n      Name: row.name\n    }\n  });\n}\nconsole.log(`Updated ${sheet.rows.length} leads`);\n```\n\nThe MCP client intercepts the data and tokenizes PII before it reaches the model:\n\n```\n// What the agent would see, if it logged the sheet.rows:\n[\n  { salesforceId: '00Q...', email: '[EMAIL_1]', phone: '[PHONE_1]', name: '[NAME_1]' },\n  { salesforceId: '00Q...', email: '[EMAIL_2]', phone: '[PHONE_2]', name: '[NAME_2]' },\n  ...\n]\n```\n\nThen, when the data is shared in another MCP tool call, it is untokenized via a lookup in the MCP client. The real email addresses, phone numbers, and names flow from Google Sheets to Salesforce, but never through the model. This prevents the agent from accidentally logging or processing sensitive data. You can also use this to define deterministic security rules, choosing where data can flow to and from.\n\nState persistence and skills\n\nCode execution with filesystem access allows agents to maintain state across operations. Agents can write intermediate results to files, enabling them to resume work and track progress:\n\n```\nconst leads = await salesforce.query({ \n  query: 'SELECT Id, Email FROM Lead LIMIT 1000' \n});\nconst csvData = leads.map(l => `${l.Id},${l.Email}`).join('\\n');\nawait fs.writeFile('./workspace/leads.csv', csvData);\n\n// Later execution picks up where it left off\nconst saved = await fs.readFile('./workspace/leads.csv', 'utf-8');\n```\n\nAgents can also persist their own code as reusable functions. Once an agent develops working code for a task, it can save that implementation for future use:\n\n```\n// In ./skills/save-sheet-as-csv.ts\nimport * as gdrive from './servers/google-drive';\nexport async function saveSheetAsCsv(sheetId: string) {\n  const data = await gdrive.getSheet({ sheetId });\n  const csv = data.map(row => row.join(',')).join('\\n');\n  await fs.writeFile(`./workspace/sheet-${sheetId}.csv`, csv);\n  return `./workspace/sheet-${sheetId}.csv`;\n}\n\n// Later, in any agent execution:\nimport { saveSheetAsCsv } from './skills/save-sheet-as-csv';\nconst csvPath = await saveSheetAsCsv('abc123');\n```\n\nThis ties in closely to the concept of Skills , folders of reusable instructions, scripts, and resources for models to improve performance on specialized tasks. Adding a SKILL.md file to these saved functions creates a structured skill that models can reference and use. Over time, this allows your agent to build a toolbox of higher-level capabilities, evolving the scaffolding that it needs to work most effectively.\n\nNote that code execution introduces its own complexity. Running agent-generated code requires a secure execution environment with appropriate sandboxing , resource limits, and monitoring. These infrastructure requirements add operational overhead and security considerations that direct tool calls avoid. The benefits of code execution-reduced token costs, lower latency, and improved tool composition-should be weighed against these implementation costs.\n\nSummary\n\nMCP provides a foundational protocol for agents to connect to many tools and systems. However, once too many servers are connected, tool definitions and results can consume excessive tokens, reducing agent efficiency.\n\nAlthough many of the problems here feel novel-context management, tool composition, state persistence-they have known solutions from software engineering. Code execution applies these established patterns to agents, letting them use familiar programming constructs to interact with MCP servers more efficiently. If you implement this approach, we encourage you to share your findings with the MCP community .\n\nAcknowledgments\n\nThis article was written by Adam Jones and Conor Kelly. Thanks to Jeremy Fox, Jerome Swannack, Stuart Ritchie, Molly Vorwerck, Matt Samuels, and Maggie Vo for feedback on drafts of this post.",
      "images": [
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/42f40f6fae9ec2d7cf2e5a98908a16d0216b91be-1000x1000.svg",
        "https://www-cdn.anthropic.com/images/4zrzovbb/website/9ecf165020005c09a22a9472cee6309555485619-1920x1080.png"
      ]
    },
    "translations": {
      "es": {
        "title": "Ejecuci\u00f3n de c\u00f3digo con MCP para agentes m\u00e1s eficientes",
        "content": "Anthropic explica c\u00f3mo combinar MCP con ejecuci\u00f3n de c\u00f3digo para reducir tokens y latencia. Claude descubre herramientas bajo demanda, filtra datos grandes fuera del modelo y solo devuelve la salida final. El enfoque incluye b\u00fasqueda diferida, orquestaci\u00f3n paralela, ejemplos de uso para evitar errores y manejo de PII fuera del contexto, logrando flujos m\u00e1s precisos y ligeros.",
        "edited": false
      },
      "ukr": {
        "title": "\u0412\u0438\u043a\u043e\u043d\u0430\u043d\u043d\u044f \u043a\u043e\u0434\u0443 \u0437 MCP \u0434\u043b\u044f \u0435\u043a\u043e\u043d\u043e\u043c\u043d\u0456\u0448\u0438\u0445 \u0430\u0433\u0435\u043d\u0442\u0456\u0432",
        "content": "Anthropic \u043f\u043e\u043a\u0430\u0437\u0443\u0454, \u044f\u043a \u043f\u043e\u0454\u0434\u043d\u0430\u0442\u0438 MCP \u0437 \u0432\u0438\u043a\u043e\u043d\u0430\u043d\u043d\u044f\u043c \u043a\u043e\u0434\u0443, \u0449\u043e\u0431 \u043d\u0435 \u043f\u0435\u0440\u0435\u043d\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0443\u0432\u0430\u0442\u0438 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442. Claude \u0448\u0443\u043a\u0430\u0454 \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0438 \u0437\u0430 \u043f\u043e\u0442\u0440\u0435\u0431\u043e\u044e, \u043e\u0431\u0440\u043e\u0431\u043b\u044f\u0454 \u0432\u0435\u043b\u0438\u043a\u0456 \u0434\u0430\u043d\u0456 \u043f\u043e\u0437\u0430 \u043c\u043e\u0434\u0435\u043b\u043b\u044e \u0439 \u043f\u043e\u0432\u0435\u0440\u0442\u0430\u0454 \u043b\u0438\u0448\u0435 \u0444\u0456\u043d\u0430\u043b\u044c\u043d\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u0412\u0456\u0434\u043a\u043b\u0430\u0434\u0435\u043d\u0435 \u0437\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0435\u043d\u043d\u044f, \u043f\u0430\u0440\u0430\u043b\u0435\u043b\u044c\u043d\u0456 \u0432\u0438\u043a\u043b\u0438\u043a\u0438, \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u0438 \u0434\u043b\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0456\u0432 \u0456 \u0440\u043e\u0431\u043e\u0442\u0430 \u0437 PII \u043f\u043e\u0437\u0430 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u043e\u043c \u0437\u043c\u0435\u043d\u0448\u0443\u044e\u0442\u044c \u0442\u043e\u043a\u0435\u043d\u0438 \u0439 \u0437\u0430\u0442\u0440\u0438\u043c\u043a\u0438 \u0442\u0430 \u043f\u0456\u0434\u0432\u0438\u0449\u0443\u044e\u0442\u044c \u0442\u043e\u0447\u043d\u0456\u0441\u0442\u044c.",
        "edited": false
      }
    },
    "metadata": {
      "tags": [
        "Anthropic",
        "MCP",
        "Engineering"
      ]
    }
  },
  {
    "id": "sample-arxiv-decision-making-agents",
    "source": "arXiv",
    "source_url": "https://arxiv.org/abs/2512.10937v1",
    "title": "On Decision-Making Agents and Higher-Order Causal Processes",
    "author": "arXiv",
    "published_date": "2025-12-11T00:00:00Z",
    "scraped_at": 1733875200000,
    "status": "published",
    "content": {
      "original_html": "",
      "text": "We establish a precise correspondence between decision-making agents in partially observable\nMarkov decision processes (POMDPs) and one-input process functions, the classical limit of higherorder quantum operations. In this identification an agent’s policy and memory update combine into\na process function w that interacts with a POMDP environment via the link product. This suggests\na dual interpretation: in the physics view, the process function acts as the environment into which\nlocal operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and\nthe inserted functions represent environments",
      "images": []
    },
    "translations": {
      "es": {
        "title": "",
        "content": "Establecemos una correspondencia precisa entre agentes de toma de decisiones en procesos de decisión de Markov parcialmente observables (POMDP) y funciones de proceso de una sola entrada, el límite clásico de las operaciones cuánticas de orden superior. En esta identificación, la política del agente y la actualización de su memoria se combinan en una función de proceso w que interactúa con un entorno POMDP mediante el producto de enlace. Esto sugiere una interpretación dual: desde la perspectiva de la física, la función de proceso actúa como el entorno en el que se insertan operaciones locales (intervenciones del agente), mientras que desde la perspectiva de la IA codifica al agente y las funciones insertadas representan los entornos.",
        "edited": false
      },
      "ukr": {
        "title": "",
        "content": "Ми встановлюємо точну відповідність між агентами прийняття рішень у частково спостережуваних марковських процесах прийняття рішень (POMDP) та одновхідними функціями процесу — класичною межею квантових операцій вищого порядку. У цій ідентифікації політика агента та оновлення його пам’яті поєднуються у функцію процесу w, яка взаємодіє з середовищем POMDP через лінковий добуток. Це вказує на подвійну інтерпретацію: з точки зору фізики функція процесу виступає як середовище, у яке вставляються локальні операції (втручання агента), тоді як з точки зору ШІ вона кодує самого агента, а вставлені функції представляють середовища.",
        "edited": false
      }
    },
    "metadata": {
      "tags": [
        "arXiv",
        "Causality",
        "Agents"
      ]
    }
  }
]
